[
  {
    "paper": {
      "id": "2601.18067",
      "authors": [
        {
          "_id": "69790825df44b75fa47e4691",
          "user": {
            "_id": "65d067194c89a331b365ca0f",
            "avatarUrl": "/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg",
            "isPro": false,
            "fullname": "weipo hsin",
            "user": "weiber2002",
            "type": "user"
          },
          "name": "Wei-Po Hsin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:28:46.696Z",
          "hidden": false
        },
        {
          "_id": "69790825df44b75fa47e4692",
          "name": "Ren-Hao Deng",
          "hidden": false
        },
        {
          "_id": "69790825df44b75fa47e4693",
          "name": "Yao-Ting Hsieh",
          "hidden": false
        },
        {
          "_id": "69790825df44b75fa47e4694",
          "name": "En-Ming Huang",
          "hidden": false
        },
        {
          "_id": "69790825df44b75fa47e4695",
          "name": "Shih-Hao Hung",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T01:53:54.000Z",
      "submittedOnDailyAt": "2026-01-28T10:29:39.930Z",
      "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
      "submittedOnDailyBy": {
        "_id": "65d067194c89a331b365ca0f",
        "avatarUrl": "/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg",
        "isPro": false,
        "fullname": "weipo hsin",
        "user": "weiber2002",
        "type": "user"
      },
      "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.",
      "upvotes": 0,
      "discussionId": "69790825df44b75fa47e4696",
      "githubRepo": "https://github.com/weiber2002/ICRTL",
      "githubRepoAddedBy": "admin",
      "githubStars": 1,
      "organization": {
        "_id": "673248e121823ee4ea594099",
        "name": "nationaltaiwan",
        "fullname": "台灣大學",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
      }
    },
    "publishedAt": "2026-01-25T20:53:54.000Z",
    "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
    "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18067.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d067194c89a331b365ca0f",
      "avatarUrl": "/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg",
      "fullname": "weipo hsin",
      "name": "weiber2002",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "673248e121823ee4ea594099",
      "name": "nationaltaiwan",
      "fullname": "台灣大學",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.19895",
      "authors": [
        {
          "_id": "697990bddf44b75fa47e4827",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "697990bddf44b75fa47e4828",
          "user": {
            "_id": "69798a0962e9590654013cab",
            "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "laiwei-seed",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:14:04.627Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-27T18:58:46.000Z",
      "submittedOnDailyAt": "2026-01-28T08:48:49.669Z",
      "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
      "submittedOnDailyBy": {
        "_id": "69798a0962e9590654013cab",
        "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "laiwei-seed",
        "type": "user"
      },
      "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.",
      "upvotes": 4,
      "discussionId": "697990bedf44b75fa47e4829",
      "ai_summary": "A novel Post-LayerNorm Transformer architecture called Keel addresses training instability in extremely deep networks by replacing residual connections with Highway-style connections, enabling stable training beyond 1000 layers.",
      "ai_keywords": [
        "Post-LayerNorm",
        "Pre-LayerNorm",
        "Transformer architectures",
        "residual pathways",
        "gradient vanishing",
        "Highway-style connections",
        "deep learning",
        "neural network training",
        "depth scaling",
        "perplexity"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-01-27T13:58:46.000Z",
    "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
    "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19895.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "69798a0962e9590654013cab",
      "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg",
      "fullname": "Lai Wei",
      "name": "laiwei-seed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.19532",
      "authors": [
        {
          "_id": "6979ec22df44b75fa47e4920",
          "user": {
            "_id": "685c115b18f9b3926bcf1070",
            "avatarUrl": "/avatars/1051de2673e0aa9378d31c829043ad13.svg",
            "isPro": false,
            "fullname": "Marthe Ballon",
            "user": "martheballon",
            "type": "user"
          },
          "name": "Marthe Ballon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:12:21.981Z",
          "hidden": false
        },
        {
          "_id": "6979ec22df44b75fa47e4921",
          "name": "Andres Algaba",
          "hidden": false
        },
        {
          "_id": "6979ec22df44b75fa47e4922",
          "name": "Brecht Verbeken",
          "hidden": false
        },
        {
          "_id": "6979ec22df44b75fa47e4923",
          "name": "Vincent Ginis",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-27T12:20:44.000Z",
      "submittedOnDailyAt": "2026-01-28T08:33:43.569Z",
      "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
      "submittedOnDailyBy": {
        "_id": "685c115b18f9b3926bcf1070",
        "avatarUrl": "/avatars/1051de2673e0aa9378d31c829043ad13.svg",
        "isPro": false,
        "fullname": "Marthe Ballon",
        "user": "martheballon",
        "type": "user"
      },
      "summary": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset (n{=}4181) and a tagged, non-standard subset (n{=}247). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in 96.4% of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.",
      "upvotes": 0,
      "discussionId": "6979ec23df44b75fa47e4924",
      "ai_summary": "A revised mathematical benchmark dataset was created through manual auditing to reduce noise and improve model performance assessment accuracy.",
      "ai_keywords": [
        "Large Language Models",
        "Omni-MATH dataset",
        "benchmark evaluation",
        "dataset cleaning",
        "judge reliability",
        "model performance assessment"
      ]
    },
    "publishedAt": "2026-01-27T07:20:44.000Z",
    "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
    "summary": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset (n{=}4181) and a tagged, non-standard subset (n{=}247). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in 96.4% of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "685c115b18f9b3926bcf1070",
      "avatarUrl": "/avatars/1051de2673e0aa9378d31c829043ad13.svg",
      "fullname": "Marthe Ballon",
      "name": "martheballon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18724",
      "authors": [
        {
          "_id": "6979b424df44b75fa47e487a",
          "name": "Yusuke Sakai",
          "hidden": false
        },
        {
          "_id": "6979b424df44b75fa47e487b",
          "name": "Hidetaka Kamigaito",
          "hidden": false
        },
        {
          "_id": "6979b424df44b75fa47e487c",
          "name": "Taro Watanabe",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T17:48:23.000Z",
      "submittedOnDailyAt": "2026-01-28T08:11:22.031Z",
      "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
      "submittedOnDailyBy": {
        "_id": "6508463c423b46492eec64e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508463c423b46492eec64e2/WSU7NSqjk92Pr2xUIWjCk.png",
        "isPro": false,
        "fullname": "Penghui Yang",
        "user": "phyang",
        "type": "user"
      },
      "summary": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.",
      "upvotes": 3,
      "discussionId": "6979b425df44b75fa47e487d",
      "ai_summary": "Hallucinated citations, defined as false references to non-existent works, are prevalent in recent NLP conference publications, with significant implications for scientific reliability and conference credibility.",
      "ai_keywords": [""]
    },
    "publishedAt": "2026-01-26T12:48:23.000Z",
    "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
    "summary": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18724.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6508463c423b46492eec64e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508463c423b46492eec64e2/WSU7NSqjk92Pr2xUIWjCk.png",
      "fullname": "Penghui Yang",
      "name": "phyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18116",
      "authors": [
        {
          "_id": "6979d078df44b75fa47e48cd",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:12:43.095Z",
          "hidden": false
        },
        {
          "_id": "6979d078df44b75fa47e48ce",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "6979d078df44b75fa47e48cf",
          "name": "Jingang Huang",
          "hidden": false
        },
        {
          "_id": "6979d078df44b75fa47e48d0",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "6979d078df44b75fa47e48d1",
          "name": "Zhengwei Cheng",
          "hidden": false
        },
        {
          "_id": "6979d078df44b75fa47e48d2",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T04:00:56.000Z",
      "submittedOnDailyAt": "2026-01-28T06:55:03.102Z",
      "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
      "submittedOnDailyBy": {
        "_id": "632c30576bcb864974cc40a8",
        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
        "isPro": false,
        "fullname": "sunlin",
        "user": "lincharliesun",
        "type": "user"
      },
      "summary": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
      "upvotes": 9,
      "discussionId": "6979d078df44b75fa47e48d3",
      "ai_summary": "FABLE is a forest-based adaptive bi-path retrieval framework that enhances LLM-based information retrieval through hierarchical indexing and structured evidence acquisition, achieving superior performance with reduced token usage compared to traditional RAG methods.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "long-context Large Language Models",
        "hierarchical forest indexes",
        "bi-path strategy",
        "LLM-guided hierarchical traversal",
        "structure-aware propagation",
        "multi-granularity semantic structures",
        "token reduction"
      ],
      "organization": {
        "_id": "6606990280543d0b74d38438",
        "name": "qihoo360",
        "fullname": "北京奇虎科技有限公司",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
      }
    },
    "publishedAt": "2026-01-25T23:00:56.000Z",
    "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
    "summary": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c30576bcb864974cc40a8",
      "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
      "fullname": "sunlin",
      "name": "lincharliesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6606990280543d0b74d38438",
      "name": "qihoo360",
      "fullname": "北京奇虎科技有限公司",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18292",
      "authors": [
        {
          "_id": "6979d0aedf44b75fa47e48d5",
          "name": "Zhewen Tan",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48d6",
          "name": "Wenhan Yu",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48d7",
          "name": "Jianfeng Si",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48d8",
          "name": "Tongxin Liu",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48d9",
          "name": "Kaiqi Guan",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48da",
          "name": "Huiyan Jin",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48db",
          "name": "Jiawen Tao",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48dc",
          "name": "Xiaokun Yuan",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48dd",
          "name": "Duohe Ma",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48de",
          "name": "Xiangzheng Zhang",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48df",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "6979d0aedf44b75fa47e48e0",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:12:39.873Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"
      ],
      "publishedAt": "2026-01-26T09:21:43.000Z",
      "submittedOnDailyAt": "2026-01-28T06:53:04.070Z",
      "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
      "submittedOnDailyBy": {
        "_id": "632c30576bcb864974cc40a8",
        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
        "isPro": false,
        "fullname": "sunlin",
        "user": "lincharliesun",
        "type": "user"
      },
      "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.",
      "upvotes": 9,
      "discussionId": "6979d0aedf44b75fa47e48e1",
      "ai_summary": "A closed-loop reinforcement learning framework enables iterative collaboration between attacker, defender, and evaluator roles for improved large language model safety alignment without manual annotations.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "safety alignment",
        "adversarial prompt generation",
        "safety defense",
        "response assessment",
        "closed-loop framework",
        "co-improving collaboration",
        "iterative learning"
      ],
      "organization": {
        "_id": "6606990280543d0b74d38438",
        "name": "qihoo360",
        "fullname": "北京奇虎科技有限公司",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
      }
    },
    "publishedAt": "2026-01-26T04:21:43.000Z",
    "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
    "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18292.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c30576bcb864974cc40a8",
      "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
      "fullname": "sunlin",
      "name": "lincharliesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6606990280543d0b74d38438",
      "name": "qihoo360",
      "fullname": "北京奇虎科技有限公司",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.09150",
      "authors": [
        {
          "_id": "6979a32cdf44b75fa47e4831",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4832",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4833",
          "name": "Kaining Ying",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4834",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4835",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4836",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4837",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4838",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e4839",
          "name": "Yu Dai",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e483a",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "6979a32cdf44b75fa47e483b",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"
      ],
      "publishedAt": "2026-01-14T04:45:05.000Z",
      "submittedOnDailyAt": "2026-01-28T03:30:08.387Z",
      "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.",
      "upvotes": 15,
      "discussionId": "6979a32cdf44b75fa47e483c",
      "githubRepo": "https://github.com/HerzogFL/World-Craft",
      "githubRepoAddedBy": "user",
      "ai_summary": "World Craft enables non-expert users to create executable and visualizable AI environments through textual descriptions by combining structured scaffolding and multi-agent intent analysis.",
      "ai_keywords": [
        "generative agent simulation",
        "AI Town",
        "large language models",
        "agentic world creation framework",
        "world scaffold",
        "world guild",
        "multi-agent framework",
        "scene construction",
        "narrative intent conveyance"
      ],
      "githubStars": 24,
      "organization": {
        "_id": "689f08c50df4fcf7fddc0b08",
        "name": "ShandaAI",
        "fullname": "Shanda AI Research Tokyo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"
      }
    },
    "publishedAt": "2026-01-13T23:45:05.000Z",
    "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text",
    "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09150.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "689f08c50df4fcf7fddc0b08",
      "name": "ShandaAI",
      "fullname": "Shanda AI Research Tokyo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18692",
      "authors": [
        {
          "_id": "697989f6df44b75fa47e4803",
          "user": {
            "_id": "69773d4ee6878183fb90a8c7",
            "avatarUrl": "/avatars/3e9e4081e3beaf3f69c380387b8ee4c2.svg",
            "isPro": false,
            "fullname": "Wei Wu",
            "user": "Weiww99",
            "type": "user"
          },
          "name": "Wei Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:15:53.890Z",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4804",
          "name": "Fan Lu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4805",
          "name": "Yunnan Wang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4806",
          "user": {
            "_id": "64548f6c363bb3aaf9cba136",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
            "isPro": false,
            "fullname": "Shuai Yang",
            "user": "ShuaiYang03",
            "type": "user"
          },
          "name": "Shuai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:14:06.626Z",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4807",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4808",
          "name": "Fangjing Wang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4809",
          "name": "Qian Zhu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480a",
          "user": {
            "_id": "68234b1167281062097fc34e",
            "avatarUrl": "/avatars/20ab2a15f27035c8b23d9a123e5ce22c.svg",
            "isPro": false,
            "fullname": "HeSun",
            "user": "he777771",
            "type": "user"
          },
          "name": "He Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:14:08.792Z",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480b",
          "name": "Yong Wang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480c",
          "name": "Shuailei Ma",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480d",
          "name": "Yiyu Ren",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480e",
          "name": "Kejia Zhang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e480f",
          "name": "Hui Yu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4810",
          "name": "Jingmei Zhao",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4811",
          "name": "Shuai Zhou",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4812",
          "name": "Zhenqi Qiu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4813",
          "name": "Houlong Xiong",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4814",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4815",
          "name": "Zechen Wang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4816",
          "name": "Ran Cheng",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4817",
          "name": "Yong-Lu Li",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4818",
          "name": "Yongtao Huang",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e4819",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e481a",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "697989f6df44b75fa47e481b",
          "name": "Kecheng Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T17:08:04.000Z",
      "submittedOnDailyAt": "2026-01-28T03:13:43.716Z",
      "title": "A Pragmatic VLA Foundation Model",
      "submittedOnDailyBy": {
        "_id": "64252045a4f3051f54dd1d53",
        "avatarUrl": "/avatars/0e423a3291091be3b4736a14da3ce495.svg",
        "isPro": false,
        "fullname": "kecheng zheng",
        "user": "zkcys001",
        "type": "user"
      },
      "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.",
      "upvotes": 23,
      "discussionId": "697989f7df44b75fa47e481c",
      "projectPage": "https://technology.robbyant.com/lingbot-vla",
      "githubRepo": "https://github.com/robbyant/lingbot-vla",
      "githubRepoAddedBy": "auto",
      "ai_summary": "A Vision-Language-Action model trained on extensive real-world robotic data demonstrates superior performance and generalization across multiple platforms while offering enhanced efficiency through optimized training infrastructure.",
      "ai_keywords": [
        "Vision-Language-Action",
        "real-world data",
        "robotic manipulation",
        "generalization",
        "efficient codebase",
        "throughput",
        "VLA-oriented codebases"
      ],
      "githubStars": 218,
      "organization": {
        "_id": "69709f892cd08371c1011a2e",
        "name": "robbyant",
        "fullname": "Robbyant",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"
      }
    },
    "publishedAt": "2026-01-26T12:08:04.000Z",
    "title": "A Pragmatic VLA Foundation Model",
    "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64252045a4f3051f54dd1d53",
      "avatarUrl": "/avatars/0e423a3291091be3b4736a14da3ce495.svg",
      "fullname": "kecheng zheng",
      "name": "zkcys001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69709f892cd08371c1011a2e",
      "name": "robbyant",
      "fullname": "Robbyant",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18631",
      "authors": [
        {
          "_id": "6978a169026bdf0473117088",
          "user": {
            "_id": "66aca01e33f6b27979856f6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
            "isPro": false,
            "fullname": "Mingyang Song",
            "user": "hitsmy",
            "type": "user"
          },
          "name": "Mingyang Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:29:30.581Z",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf0473117089",
          "user": {
            "_id": "63a2a51ef30c464227924fc6",
            "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
            "isPro": false,
            "fullname": "Haoyu Sun",
            "user": "Mikivis",
            "type": "user"
          },
          "name": "Haoyu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:29:26.154Z",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf047311708a",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Jiawei Gu",
            "user": "kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:29:28.392Z",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf047311708b",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf047311708c",
          "name": "Luxin Xu",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf047311708d",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "6978a169026bdf047311708e",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T16:04:43.000Z",
      "submittedOnDailyAt": "2026-01-28T01:52:20.218Z",
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "submittedOnDailyBy": {
        "_id": "66aca01e33f6b27979856f6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
        "isPro": false,
        "fullname": "Mingyang Song",
        "user": "hitsmy",
        "type": "user"
      },
      "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
      "upvotes": 37,
      "discussionId": "6978a16a026bdf047311708f",
      "projectPage": "https://adareasoner.github.io/",
      "githubRepo": "https://github.com/ssmisya/AdaReasoner",
      "githubRepoAddedBy": "user",
      "ai_summary": "AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "tool use",
        "reinforcement learning",
        "end-task success",
        "adaptive learning mechanism",
        "visual reasoning",
        "multimodal models",
        "tool selection",
        "tool sequencing",
        "long-horizon interactions"
      ],
      "githubStars": 42,
      "organization": {
        "_id": "643cb0625fcffe09fb6ca688",
        "name": "Fudan-University",
        "fullname": "Fudan University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
      }
    },
    "publishedAt": "2026-01-26T11:04:43.000Z",
    "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66aca01e33f6b27979856f6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
      "fullname": "Mingyang Song",
      "name": "hitsmy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "643cb0625fcffe09fb6ca688",
      "name": "Fudan-University",
      "fullname": "Fudan University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18491",
      "authors": [
        {
          "_id": "697831d9026bdf0473116e5c",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e5d",
          "user": {
            "_id": "66e2624a436a1798365e4581",
            "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
            "isPro": false,
            "fullname": "Qihan Ren",
            "user": "jasonrqh",
            "type": "user"
          },
          "name": "Qihan Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:31:15.765Z",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e5e",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e5f",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e60",
          "name": "Yuejin Xie",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e61",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e62",
          "name": "Zhonghao Yang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e63",
          "name": "Haoyu Luo",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e64",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e65",
          "name": "Qingyu Liu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e66",
          "name": "Binxin Hu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e67",
          "name": "Ling Tang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e68",
          "name": "Jilin Mei",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e69",
          "name": "Dadi Guo",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6a",
          "name": "Leitao Yuan",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6b",
          "name": "Junyao Yang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6c",
          "name": "Guanxu Chen",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6d",
          "name": "Qihao Lin",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6e",
          "name": "Yi Yu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e6f",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e70",
          "name": "Jiaxuan Guo",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e71",
          "name": "Jie Zhang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e72",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e73",
          "name": "Huiqi Deng",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e74",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e75",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e76",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e77",
          "name": "Wen Shen",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e78",
          "name": "Zhikai Chen",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e79",
          "name": "Haoyu Xie",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7a",
          "name": "Jialing Tao",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7b",
          "name": "Juntao Dai",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7c",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7d",
          "name": "Zhongjie Ba",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7e",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e7f",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e80",
          "name": "Quanshi Zhang",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e81",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e82",
          "name": "Zhihua Wei",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e83",
          "name": "Hui Xue",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e84",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e85",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "697831d9026bdf0473116e86",
          "name": "Xia Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T13:45:41.000Z",
      "submittedOnDailyAt": "2026-01-28T01:26:49.833Z",
      "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
      "submittedOnDailyBy": {
        "_id": "66e2624a436a1798365e4581",
        "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
        "isPro": false,
        "fullname": "Qihan Ren",
        "user": "jasonrqh",
        "type": "user"
      },
      "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
      "upvotes": 55,
      "discussionId": "697831d9026bdf0473116e87",
      "githubRepo": "https://github.com/AI45Lab/AgentDoG",
      "githubRepoAddedBy": "user",
      "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.",
      "ai_keywords": [
        "agentic guardrail",
        "three-dimensional taxonomy",
        "agentic safety benchmark",
        "Diagnostic Guardrail framework",
        "agent safety and security",
        "agent trajectories",
        "root cause diagnosis",
        "fine-grained monitoring",
        "model variants",
        "state-of-the-art performance"
      ],
      "githubStars": 174,
      "organization": {
        "_id": "68f716f832b31e42cbc2be7f",
        "name": "AI45Research",
        "fullname": "AI45Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"
      }
    },
    "publishedAt": "2026-01-26T08:45:41.000Z",
    "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
    "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "66e2624a436a1798365e4581",
      "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
      "fullname": "Qihan Ren",
      "name": "jasonrqh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68f716f832b31e42cbc2be7f",
      "name": "AI45Research",
      "fullname": "AI45Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.19149",
      "authors": [
        {
          "_id": "697986c2df44b75fa47e47e7",
          "user": {
            "_id": "6310a812a23f0327bce68778",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
            "isPro": false,
            "fullname": "Ethan Ning",
            "user": "ethanning",
            "type": "user"
          },
          "name": "Jingjie Ning",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:15:58.913Z",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47e8",
          "name": "Xiangzhen Shen",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47e9",
          "name": "Li Hou",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47ea",
          "name": "Shiyi Shen",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47eb",
          "name": "Jiahao Yang",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47ec",
          "name": "Junrui Li",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47ed",
          "name": "Hong Shan",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47ee",
          "name": "Sanan Wu",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47ef",
          "name": "Sihan Gao",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47f0",
          "name": "Huaqiang Eric Xu",
          "hidden": false
        },
        {
          "_id": "697986c2df44b75fa47e47f1",
          "name": "Xinheng He",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-27T03:27:04.000Z",
      "submittedOnDailyAt": "2026-01-28T01:17:38.797Z",
      "title": "GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery",
      "submittedOnDailyBy": {
        "_id": "6310a812a23f0327bce68778",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
        "isPro": false,
        "fullname": "Ethan Ning",
        "user": "ethanning",
        "type": "user"
      },
      "summary": "G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems.",
      "upvotes": 1,
      "discussionId": "697986c2df44b75fa47e47f2",
      "ai_summary": "GPCR-Filter is a deep learning framework that combines protein language models and graph neural networks to identify GPCR modulators with high accuracy and generalization across unseen receptors and ligands.",
      "ai_keywords": [
        "GPCR",
        "deep learning framework",
        "ESM-3 protein language model",
        "graph neural networks",
        "attention-based fusion mechanism",
        "compound-protein interaction models",
        "receptor-ligand functional relationships"
      ],
      "organization": {
        "_id": "691d9a1012cc4d473e1c862f",
        "name": "CarnegieMellonU",
        "fullname": "Carnegie Mellon University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
      }
    },
    "publishedAt": "2026-01-26T22:27:04.000Z",
    "title": "GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery",
    "summary": "G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a812a23f0327bce68778",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
      "fullname": "Ethan Ning",
      "name": "ethanning",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "691d9a1012cc4d473e1c862f",
      "name": "CarnegieMellonU",
      "fullname": "Carnegie Mellon University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.19375",
      "authors": [
        {
          "_id": "697981b2df44b75fa47e47a5",
          "user": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "isPro": false,
            "fullname": "Quy-Anh Dang",
            "user": "quyanh",
            "type": "user"
          },
          "name": "Quy-Anh Dang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:16:05.195Z",
          "hidden": false
        },
        {
          "_id": "697981b2df44b75fa47e47a6",
          "name": "Chris Ngo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-27T08:56:25.000Z",
      "submittedOnDailyAt": "2026-01-28T01:02:05.884Z",
      "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering",
      "upvotes": 4,
      "discussionId": "697981b2df44b75fa47e47a7",
      "projectPage": "https://knoveleng.github.io/steering/",
      "githubRepo": "https://github.com/knoveleng/steering",
      "githubRepoAddedBy": "user",
      "ai_summary": "Selective Steering enables continuous, norm-preserving control of language model behavior through targeted layer selection and mathematically rigorous rotation techniques.",
      "ai_keywords": [
        "activation steering",
        "angular steering",
        "norm preservation",
        "layer selection",
        "feature representations",
        "activation distribution integrity",
        "attack success rates",
        "perplexity violations",
        "capability retention",
        "language models"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "6735b5052769638944f432c9",
        "name": "knoveleng",
        "fullname": "Knovel Engineering",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"
      }
    },
    "publishedAt": "2026-01-27T03:56:25.000Z",
    "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
    "summary": "Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6735b5052769638944f432c9",
      "name": "knoveleng",
      "fullname": "Knovel Engineering",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.19834",
      "authors": [
        {
          "_id": "69797e24df44b75fa47e4761",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:16:37.217Z",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4762",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4763",
          "name": "Hongyi Yuan",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4764",
          "name": "Xiangcheng Zhang",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4765",
          "name": "Tianhao Huang",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4766",
          "name": "Changjing He",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4767",
          "name": "Chaoyi Deng",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4768",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e4769",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "69797e24df44b75fa47e476a",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-27T17:40:07.000Z",
      "submittedOnDailyAt": "2026-01-28T00:46:44.173Z",
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "submittedOnDailyBy": {
        "_id": "643b866bff50448bcfc7d1d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "manchery",
        "type": "user"
      },
      "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
      "upvotes": 16,
      "discussionId": "69797e24df44b75fa47e476b",
      "projectPage": "https://thuml.github.io/Reasoning-Visual-World/",
      "githubRepo": "https://github.com/thuml/Reasoning-Visual-World",
      "githubRepoAddedBy": "user",
      "ai_summary": "Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "large language models",
        "multimodal models",
        "visual generation",
        "world models",
        "visual superiority hypothesis",
        "internal world modeling",
        "interleaved reasoning",
        "VisWorld-Eval"
      ],
      "githubStars": 31,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-01-27T12:40:07.000Z",
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b866bff50448bcfc7d1d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
      "fullname": "Jialong Wu",
      "name": "manchery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17645",
      "authors": [
        {
          "_id": "6978296a026bdf0473116dc0",
          "name": "Xilin Jiang",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc1",
          "user": {
            "_id": "68fc33d3705bf8aa76ef5215",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8kks5zJgOMoU2GuXUgRZq.jpeg",
            "isPro": true,
            "fullname": "Qiaolin Wang",
            "user": "QiaolinWang",
            "type": "user"
          },
          "name": "Qiaolin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:31:22.668Z",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc2",
          "user": {
            "_id": "665867a5d258e0fe41dcfb7a",
            "avatarUrl": "/avatars/56bab52d53f4bdd1cba3c11599c3372e.svg",
            "isPro": false,
            "fullname": "Junkai Wu",
            "user": "wjk0925",
            "type": "user"
          },
          "name": "Junkai Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:42:46.216Z",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc3",
          "name": "Xiaomin He",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc4",
          "name": "Zhongweiyang Xu",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc5",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc6",
          "name": "Minshuo Piao",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc7",
          "name": "Kaiyi Yang",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc8",
          "name": "Xiuwen Zheng",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dc9",
          "name": "Riki Shimizu",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dca",
          "name": "Yicong Chen",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dcb",
          "name": "Arsalan Firoozi",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dcc",
          "name": "Gavin Mischler",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dcd",
          "name": "Sukru Samet Dindar",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dce",
          "name": "Richard Antonello",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dcf",
          "name": "Linyang He",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd0",
          "name": "Tsun-An Hsieh",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd1",
          "name": "Xulin Fan",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd2",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd3",
          "name": "Yuesheng Ma",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd4",
          "name": "Chaitanya Amballa",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd5",
          "name": "Weixiong Chen",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd6",
          "name": "Jiarui Hai",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd7",
          "name": "Ruisi Li",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd8",
          "name": "Vishal Choudhari",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dd9",
          "name": "Cong Han",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dda",
          "name": "Yinghao Aaron Li",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116ddb",
          "name": "Adeen Flinker",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116ddc",
          "name": "Mounya Elhilali",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116ddd",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116dde",
          "name": "Mark Hasegawa-Johnson",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116ddf",
          "name": "Romit Roy Choudhury",
          "hidden": false
        },
        {
          "_id": "6978296a026bdf0473116de0",
          "name": "Nima Mesgarani",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-25T01:40:15.000Z",
      "submittedOnDailyAt": "2026-01-28T00:44:43.594Z",
      "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
      "submittedOnDailyBy": {
        "_id": "6531a65daed617662c7f1007",
        "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
        "isPro": false,
        "fullname": "Xilin Jiang",
        "user": "xi-j",
        "type": "user"
      },
      "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
      "upvotes": 13,
      "discussionId": "6978296b026bdf0473116de1",
      "projectPage": "https://avmemeexam.github.io/public",
      "ai_summary": "Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.",
      "ai_keywords": [
        "multimodal large language models",
        "audio-visual clips",
        "human-curated benchmark",
        "iconic Internet sounds",
        "cultural context",
        "surface content",
        "semantic comprehension"
      ],
      "organization": {
        "_id": "63f68badb607296857bb2441",
        "name": "columbia",
        "fullname": "Columbia University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"
      }
    },
    "publishedAt": "2026-01-24T20:40:15.000Z",
    "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
    "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6531a65daed617662c7f1007",
      "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
      "fullname": "Xilin Jiang",
      "name": "xi-j",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63f68badb607296857bb2441",
      "name": "columbia",
      "fullname": "Columbia University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.19362",
      "authors": [
        {
          "_id": "69797926df44b75fa47e4748",
          "user": {
            "_id": "63510eea0b94548566dad923",
            "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
            "isPro": false,
            "fullname": "Xinyi Wan",
            "user": "ufotalent",
            "type": "user"
          },
          "name": "Xinyi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:16:39.482Z",
          "hidden": false
        },
        {
          "_id": "69797926df44b75fa47e4749",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "69797926df44b75fa47e474a",
          "name": "Guangxing Huang",
          "hidden": false
        },
        {
          "_id": "69797926df44b75fa47e474b",
          "name": "Chaoyi Ruan",
          "hidden": false
        },
        {
          "_id": "69797926df44b75fa47e474c",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "69797926df44b75fa47e474d",
          "name": "Jialin Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/W0qTS4_tXdhIrKSgx7r9m.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/CGtDjIW1W23_uGvOEXCLr.png"
      ],
      "publishedAt": "2026-01-27T08:44:46.000Z",
      "submittedOnDailyAt": "2026-01-28T00:33:12.158Z",
      "title": "Revisiting Parameter Server in LLM Post-Training",
      "submittedOnDailyBy": {
        "_id": "63510eea0b94548566dad923",
        "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
        "isPro": false,
        "fullname": "Xinyi Wan",
        "user": "ufotalent",
        "type": "user"
      },
      "summary": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.",
      "upvotes": 3,
      "discussionId": "69797926df44b75fa47e474e",
      "githubRepo": "https://github.com/sail-sg/odc",
      "githubRepoAddedBy": "user",
      "ai_summary": "On-Demand Communication (ODC) adapts parameter server principles to Fully Sharded Data Parallel training by replacing collective communication with point-to-point communication, improving device utilization and throughput in imbalanced large language model training scenarios.",
      "ai_keywords": [
        "data parallel",
        "parameter servers",
        "collective communication",
        "Fully Sharded Data Parallel",
        "On-Demand Communication",
        "all-gather",
        "reduce-scatter",
        "point-to-point communication",
        "load balancing",
        "device utilization",
        "training throughput"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "61f4e841c771e23a1abb61ff",
        "name": "sail",
        "fullname": "Sea AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
      }
    },
    "publishedAt": "2026-01-27T03:44:46.000Z",
    "title": "Revisiting Parameter Server in LLM Post-Training",
    "summary": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/W0qTS4_tXdhIrKSgx7r9m.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/CGtDjIW1W23_uGvOEXCLr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63510eea0b94548566dad923",
      "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
      "fullname": "Xinyi Wan",
      "name": "ufotalent",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61f4e841c771e23a1abb61ff",
      "name": "sail",
      "fullname": "Sea AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17237",
      "authors": [
        {
          "_id": "6978df47026bdf047311712d",
          "name": "Mike Ranzinger",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf047311712e",
          "name": "Greg Heinrich",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf047311712f",
          "name": "Collin McCarthy",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf0473117130",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf0473117131",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf0473117132",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "6978df47026bdf0473117133",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-24T00:04:03.000Z",
      "submittedOnDailyAt": "2026-01-27T23:54:40.196Z",
      "title": "C-RADIOv4 (Tech Report)",
      "submittedOnDailyBy": {
        "_id": "610a70f35a40a8bfebfbf09b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg",
        "isPro": true,
        "fullname": "Daniel Bourke",
        "user": "mrdbourke",
        "type": "user"
      },
      "summary": "By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.",
      "upvotes": 1,
      "discussionId": "6978df48026bdf0473117134",
      "ai_summary": "Multi-teacher distillation enables unified student models that maintain and enhance multiple teacher capabilities, with C-RADIOv4 offering improved performance and efficiency through updated training teachers and enhanced resolution support.",
      "ai_keywords": [
        "multi-teacher distillation",
        "agglomerative vision backbones",
        "student model",
        "C-RADIOv4",
        "AM-RADIO",
        "RADIOv2.5",
        "SigLIP2",
        "DINOv3",
        "SAM3",
        "ViTDet"
      ]
    },
    "publishedAt": "2026-01-23T19:04:03.000Z",
    "title": "C-RADIOv4 (Tech Report)",
    "summary": "By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17237.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610a70f35a40a8bfebfbf09b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg",
      "fullname": "Daniel Bourke",
      "name": "mrdbourke",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 191,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.17668",
      "authors": [
        {
          "_id": "69796de4df44b75fa47e4711",
          "name": "Jang-Hyun Kim",
          "hidden": false
        },
        {
          "_id": "69796de4df44b75fa47e4712",
          "name": "Dongyoon Han",
          "hidden": false
        },
        {
          "_id": "69796de4df44b75fa47e4713",
          "name": "Sangdoo Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-25T03:07:54.000Z",
      "submittedOnDailyAt": "2026-01-27T23:33:57.956Z",
      "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction",
      "submittedOnDailyBy": {
        "_id": "648ac415718bc0670a5a5f56",
        "avatarUrl": "/avatars/27189e289b808ef01689ff2abb7a56bf.svg",
        "isPro": false,
        "fullname": "Sangdoo Yun",
        "user": "oodgnas",
        "type": "user"
      },
      "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.",
      "upvotes": 3,
      "discussionId": "69796de5df44b75fa47e4714",
      "ai_summary": "A novel gating-based key-value cache eviction method for frozen-weight large language models achieves high compression ratios with minimal computational overhead while maintaining near-lossless performance across diverse tasks.",
      "ai_keywords": [
        "key-value cache",
        "KV cache eviction",
        "frozen-weight LLMs",
        "sink-attention gating modules",
        "forward passes",
        "task-agnostic reconstruction objective",
        "long-context understanding",
        "code comprehension",
        "mathematical reasoning"
      ],
      "organization": {
        "_id": "64ffe603efd273eec7768bde",
        "name": "naver-ai",
        "fullname": "NAVER AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
      }
    },
    "publishedAt": "2026-01-24T22:07:54.000Z",
    "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction",
    "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17668.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac415718bc0670a5a5f56",
      "avatarUrl": "/avatars/27189e289b808ef01689ff2abb7a56bf.svg",
      "fullname": "Sangdoo Yun",
      "name": "oodgnas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64ffe603efd273eec7768bde",
      "name": "naver-ai",
      "fullname": "NAVER AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18226",
      "authors": [
        {
          "_id": "697837c6026bdf0473116ec9",
          "user": {
            "_id": "667a15813e899391285b378e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667a15813e899391285b378e/y8yywFt0hmSKSryYFVhQm.jpeg",
            "isPro": false,
            "fullname": "Haotian Li",
            "user": "lirt1231",
            "type": "user"
          },
          "name": "Haotian Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:07.956Z",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116eca",
          "name": "Shijun Yang",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ecb",
          "user": {
            "_id": "5f1d7cf0cb8f993fa01f471d",
            "avatarUrl": "/avatars/2e0f41033a63b776eff2e7dbfdc2a4cd.svg",
            "isPro": false,
            "fullname": "weizhen",
            "user": "weizhen",
            "type": "user"
          },
          "name": "Weizhen Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:11.228Z",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ecc",
          "name": "Silei Zhao",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ecd",
          "name": "Rui Hua",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ece",
          "name": "Mingzhu Song",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ecf",
          "name": "Xiaojian Yang",
          "hidden": false
        },
        {
          "_id": "697837c6026bdf0473116ed0",
          "name": "Chao Peng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/7U3s99xTxnDw3g6d-P9tI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/Xo_lBYiGFvBu3gdEgkEg-.mp4"
      ],
      "publishedAt": "2026-01-26T07:27:47.000Z",
      "submittedOnDailyAt": "2026-01-27T23:28:09.215Z",
      "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks",
      "submittedOnDailyBy": {
        "_id": "667a15813e899391285b378e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667a15813e899391285b378e/y8yywFt0hmSKSryYFVhQm.jpeg",
        "isPro": false,
        "fullname": "Haotian Li",
        "user": "lirt1231",
        "type": "user"
      },
      "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.",
      "upvotes": 4,
      "discussionId": "697837c7026bdf0473116ed1",
      "projectPage": "https://www.yunjuetech.com/en",
      "githubRepo": "https://github.com/YunjueTech/Yunjue-Agent",
      "githubRepoAddedBy": "user",
      "ai_summary": "Agents that evolve tools through continuous interaction and feedback can adapt to dynamic environments and transfer knowledge across domains more effectively than traditional systems.",
      "ai_keywords": [
        "agent systems",
        "open-ended environments",
        "task distributions",
        "tool evolution",
        "self-evolving paradigm",
        "continuous stream of experience",
        "reinforcement learning",
        "evolutionary efficiency",
        "parallel batch evolution",
        "zero-start setting",
        "warm-start evaluations",
        "convergence monitoring"
      ],
      "githubStars": 144,
      "organization": {
        "_id": "697828b20534a6dacf3d771d",
        "name": "YunjueTech",
        "fullname": "Yunjue Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/ngStMfYJNQRMehYbaVd0l.jpeg"
      }
    },
    "publishedAt": "2026-01-26T02:27:47.000Z",
    "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks",
    "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/7U3s99xTxnDw3g6d-P9tI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/Xo_lBYiGFvBu3gdEgkEg-.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18226.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667a15813e899391285b378e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667a15813e899391285b378e/y8yywFt0hmSKSryYFVhQm.jpeg",
      "fullname": "Haotian Li",
      "name": "lirt1231",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "697828b20534a6dacf3d771d",
      "name": "YunjueTech",
      "fullname": "Yunjue Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667a15813e899391285b378e/ngStMfYJNQRMehYbaVd0l.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17617",
      "authors": [
        {
          "_id": "69794dcedf44b75fa47e46ef",
          "user": {
            "_id": "6310a812a23f0327bce68778",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
            "isPro": false,
            "fullname": "Ethan Ning",
            "user": "ethanning",
            "type": "user"
          },
          "name": "Jingjie Ning",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:16:44.338Z",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f0",
          "name": "João Coelho",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f1",
          "name": "Yibo Kong",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f2",
          "name": "Yunfan Long",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f3",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f4",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f5",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "69794dcedf44b75fa47e46f6",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6310a812a23f0327bce68778/1h5zQrBlWGSaJUvkCTeV2.png"
      ],
      "publishedAt": "2026-01-24T22:42:43.000Z",
      "submittedOnDailyAt": "2026-01-27T21:30:55.811Z",
      "title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
      "submittedOnDailyBy": {
        "_id": "6310a812a23f0327bce68778",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
        "isPro": false,
        "fullname": "Ethan Ning",
        "user": "ethanning",
        "type": "user"
      },
      "summary": "LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.",
      "upvotes": 1,
      "discussionId": "69794dcedf44b75fa47e46f7",
      "ai_summary": "Large-scale analysis of LLM-powered search agent behavior reveals patterns in multi-step information seeking, evidence reuse, and session dynamics from 14.44 million search requests.",
      "ai_keywords": [
        "agentic search",
        "search agents",
        "multi-step information seeking",
        "log analysis",
        "DeepResearchGym",
        "sessionization",
        "query reformulation",
        "Context-driven Term Adoption Rate",
        "CTAR",
        "evidence reuse",
        "early stopping",
        "retrieval budgets",
        "cross-step context tracking"
      ],
      "organization": {
        "_id": "691d9a1012cc4d473e1c862f",
        "name": "CarnegieMellonU",
        "fullname": "Carnegie Mellon University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
      }
    },
    "publishedAt": "2026-01-24T17:42:43.000Z",
    "title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
    "summary": "LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6310a812a23f0327bce68778/1h5zQrBlWGSaJUvkCTeV2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6310a812a23f0327bce68778",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg",
      "fullname": "Ethan Ning",
      "name": "ethanning",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "691d9a1012cc4d473e1c862f",
      "name": "CarnegieMellonU",
      "fullname": "Carnegie Mellon University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17895",
      "authors": [
        {
          "_id": "69781d38026bdf0473116d7f",
          "name": "Bin Tan",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d80",
          "name": "Changjiang Sun",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d81",
          "name": "Xiage Qin",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d82",
          "name": "Hanat Adai",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d83",
          "name": "Zelin Fu",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d84",
          "name": "Tianxiang Zhou",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d85",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d86",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d87",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d88",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "69781d38026bdf0473116d89",
          "name": "Nan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-25T16:13:49.000Z",
      "submittedOnDailyAt": "2026-01-27T20:37:22.098Z",
      "title": "Masked Depth Modeling for Spatial Perception",
      "submittedOnDailyBy": {
        "_id": "6485ce5ec7f19728a49df17a",
        "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
        "isPro": true,
        "fullname": "Nan",
        "user": "cherubicxn",
        "type": "user"
      },
      "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.",
      "upvotes": 17,
      "discussionId": "69781d39026bdf0473116d8a",
      "projectPage": "https://technology.robbyant.com/lingbot-depth",
      "githubRepo": "https://github.com/Robbyant/lingbot-depth",
      "githubRepoAddedBy": "user",
      "ai_summary": "LingBot-Depth is a depth completion model that uses visual context to refine depth maps through masked depth modeling and automated data curation for improved spatial perception in robotics and autonomous systems.",
      "ai_keywords": [
        "depth completion",
        "masked depth modeling",
        "visual context",
        "automated data curation",
        "RGB-D cameras",
        "spatial perception",
        "latent representation"
      ],
      "githubStars": 360,
      "organization": {
        "_id": "69709f892cd08371c1011a2e",
        "name": "robbyant",
        "fullname": "Robbyant",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"
      }
    },
    "publishedAt": "2026-01-25T11:13:49.000Z",
    "title": "Masked Depth Modeling for Spatial Perception",
    "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17895.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6485ce5ec7f19728a49df17a",
      "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
      "fullname": "Nan",
      "name": "cherubicxn",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69709f892cd08371c1011a2e",
      "name": "robbyant",
      "fullname": "Robbyant",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18744",
      "authors": [
        {
          "_id": "6978e4ca026bdf0473117136",
          "user": {
            "_id": "656c2e043e60cb2621a93224",
            "avatarUrl": "/avatars/9e8b848e9c2fe0ae4fd7c0c9c772eb86.svg",
            "isPro": false,
            "fullname": "Fangxu",
            "user": "ParadiseYu",
            "type": "user"
          },
          "name": "Fangxu Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:29:23.850Z",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf0473117137",
          "name": "Xingang Guo",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf0473117138",
          "name": "Lingzhi Yuan",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf0473117139",
          "name": "Haoqiang Kang",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf047311713a",
          "name": "Hongyu Zhao",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf047311713b",
          "name": "Lianhui Qin",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf047311713c",
          "name": "Furong Huang",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf047311713d",
          "name": "Bin Hu",
          "hidden": false
        },
        {
          "_id": "6978e4ca026bdf047311713e",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T18:04:54.000Z",
      "submittedOnDailyAt": "2026-01-27T16:53:47.561Z",
      "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
      "submittedOnDailyBy": {
        "_id": "656c2e043e60cb2621a93224",
        "avatarUrl": "/avatars/9e8b848e9c2fe0ae4fd7c0c9c772eb86.svg",
        "isPro": false,
        "fullname": "Fangxu",
        "user": "ParadiseYu",
        "type": "user"
      },
      "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.",
      "upvotes": 7,
      "discussionId": "6978e4ca026bdf047311713f",
      "ai_summary": "TSRBench presents a comprehensive multi-modal benchmark for evaluating time series reasoning capabilities across perception, reasoning, prediction, and decision-making dimensions, revealing that scaling laws and reasoning abilities do not uniformly apply to time series forecasting tasks.",
      "ai_keywords": [
        "time series data",
        "generalist models",
        "multi-modal benchmark",
        "time series reasoning",
        "perception",
        "reasoning",
        "prediction",
        "decision-making",
        "large language models",
        "vision-language models",
        "time series language models",
        "scaling laws",
        "numerical reasoning",
        "context-aware forecasting",
        "multimodal models",
        "textual representations",
        "visual representations"
      ],
      "organization": {
        "_id": "68b3c3bbc375e05b059370b2",
        "name": "UMCP",
        "fullname": "University of Maryland College Park",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
      }
    },
    "publishedAt": "2026-01-26T13:04:54.000Z",
    "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
    "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18744.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656c2e043e60cb2621a93224",
      "avatarUrl": "/avatars/9e8b848e9c2fe0ae4fd7c0c9c772eb86.svg",
      "fullname": "Fangxu",
      "name": "ParadiseYu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b3c3bbc375e05b059370b2",
      "name": "UMCP",
      "fullname": "University of Maryland College Park",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.14103",
      "authors": [
        {
          "_id": "69718603c1c7409747bf9530",
          "user": {
            "_id": "66863974b9a71fa518a5eea1",
            "avatarUrl": "/avatars/9012bc92197242fec9801deff65e1e0f.svg",
            "isPro": false,
            "fullname": "Xiaolu Liu",
            "user": "xiaolul2",
            "type": "user"
          },
          "name": "Xiaolu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-22T08:45:29.814Z",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9531",
          "name": "Yicong Li",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9532",
          "name": "Qiyuan He",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9533",
          "name": "Jiayin Zhu",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9534",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9535",
          "name": "Angela Yao",
          "hidden": false
        },
        {
          "_id": "69718603c1c7409747bf9536",
          "name": "Jianke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-20T16:03:22.000Z",
      "submittedOnDailyAt": "2026-01-27T14:40:51.767Z",
      "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
      "submittedOnDailyBy": {
        "_id": "66863974b9a71fa518a5eea1",
        "avatarUrl": "/avatars/9012bc92197242fec9801deff65e1e0f.svg",
        "isPro": false,
        "fullname": "Xiaolu Liu",
        "user": "xiaolul2",
        "type": "user"
      },
      "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.",
      "upvotes": 1,
      "discussionId": "69718603c1c7409747bf9537",
      "projectPage": "https://interp3d.github.io/",
      "githubRepo": "https://github.com/xiaolul2/Interp3D",
      "githubRepoAddedBy": "user",
      "ai_summary": "Interp3D is a training-free framework for textured 3D morphing that preserves geometric consistency and texture alignment through generative priors and progressive alignment principles.",
      "ai_keywords": [
        "textured 3D morphing",
        "generative priors",
        "progressive alignment",
        "SLAT",
        "structured latent",
        "texture fusion",
        "semantically aligned interpolation",
        "structural consistency",
        "appearance transfer"
      ],
      "githubStars": 12
    },
    "publishedAt": "2026-01-20T11:03:22.000Z",
    "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
    "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14103.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66863974b9a71fa518a5eea1",
      "avatarUrl": "/avatars/9012bc92197242fec9801deff65e1e0f.svg",
      "fullname": "Xiaolu Liu",
      "name": "xiaolul2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18790",
      "authors": [
        {
          "_id": "6978d6b8026bdf0473117120",
          "name": "Etienne Lanzeray",
          "hidden": false
        },
        {
          "_id": "6978d6b8026bdf0473117121",
          "name": "Stephane Meilliez",
          "hidden": false
        },
        {
          "_id": "6978d6b8026bdf0473117122",
          "name": "Malo Ruelle",
          "hidden": false
        },
        {
          "_id": "6978d6b8026bdf0473117123",
          "name": "Damien Sileo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T18:55:07.000Z",
      "submittedOnDailyAt": "2026-01-27T12:46:35.309Z",
      "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
      "submittedOnDailyBy": {
        "_id": "5fc0bcb41160c47d1d43856b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
        "isPro": false,
        "fullname": "Damien Sileo",
        "user": "sileod",
        "type": "user"
      },
      "summary": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.",
      "upvotes": 1,
      "discussionId": "6978d6b8026bdf0473117124",
      "ai_summary": "Specialized reasoning models prioritize task completion over safety, potentially ignoring critical emergencies during complex calculations.",
      "ai_keywords": [
        "large language models",
        "deep reasoning",
        "safety",
        "mortality",
        "computational time",
        "task completion"
      ],
      "organization": {
        "_id": "65c38a0b3da1c95279966f18",
        "name": "INRIA",
        "fullname": "Institut National de Recherche en Informatique et en Automatique",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62627a439517ea567fb916f2/dEVzAgPUZD6uL0E1Rzllb.png"
      }
    },
    "publishedAt": "2026-01-26T13:55:07.000Z",
    "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
    "summary": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18790.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "5fc0bcb41160c47d1d43856b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/AHCEW4TfTdyjNBx-V_F5A.png",
      "fullname": "Damien Sileo",
      "name": "sileod",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65c38a0b3da1c95279966f18",
      "name": "INRIA",
      "fullname": "Institut National de Recherche en Informatique et en Automatique",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62627a439517ea567fb916f2/dEVzAgPUZD6uL0E1Rzllb.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18753",
      "authors": [
        {
          "_id": "6978adf7026bdf04731170a4",
          "user": {
            "_id": "66feb5c3d74232f132ea11ba",
            "avatarUrl": "/avatars/683bb03e904d85585cc5fe901b2f2d7c.svg",
            "isPro": false,
            "fullname": "Xinyue Zeng",
            "user": "xyzeng2000",
            "type": "user"
          },
          "name": "Xinyue Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T13:56:37.622Z",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170a5",
          "name": "Junhong Lin",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170a6",
          "user": {
            "_id": "63e74bd7db40d9e67feae456",
            "avatarUrl": "/avatars/572328b8a18242ab9b3ffd88a3afbd57.svg",
            "isPro": false,
            "fullname": "yanyujun",
            "user": "yanyujun",
            "type": "user"
          },
          "name": "Yujun Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:41:28.477Z",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170a7",
          "name": "Feng Guo",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170a8",
          "user": {
            "_id": "63424d32dd8853dc7e3ccf00",
            "avatarUrl": "/avatars/938df653561d8738e468f96116475350.svg",
            "isPro": false,
            "fullname": "Liang Shi",
            "user": "liangshi",
            "type": "user"
          },
          "name": "Liang Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:41:18.235Z",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170a9",
          "name": "Jun Wu",
          "hidden": false
        },
        {
          "_id": "6978adf7026bdf04731170aa",
          "user": {
            "_id": "64254ff8120a3ed323314e0c",
            "avatarUrl": "/avatars/134325256b9073a96c6d7733a84d2630.svg",
            "isPro": false,
            "fullname": "Da-Wei Zhou",
            "user": "zhoudw",
            "type": "user"
          },
          "name": "Dawei Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:41:09.140Z",
          "hidden": true
        }
      ],
      "publishedAt": "2026-01-26T18:23:09.000Z",
      "submittedOnDailyAt": "2026-01-27T10:00:30.358Z",
      "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
      "submittedOnDailyBy": {
        "_id": "66feb5c3d74232f132ea11ba",
        "avatarUrl": "/avatars/683bb03e904d85585cc5fe901b2f2d7c.svg",
        "isPro": false,
        "fullname": "Xinyue Zeng",
        "user": "xyzeng2000",
        "type": "user"
      },
      "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.",
      "upvotes": 1,
      "discussionId": "6978adf7026bdf04731170ab",
      "ai_summary": "A theoretical framework and detection method for identifying hallucinations in large language models by analyzing data-driven and reasoning-driven components through neural tangent kernel-based scoring.",
      "ai_keywords": [
        "Hallucination Risk Bound",
        "Neural Tangent Kernel",
        "NTK-based score",
        "data-driven hallucinations",
        "reasoning-driven hallucinations",
        "large language models",
        "hallucination detection"
      ]
    },
    "publishedAt": "2026-01-26T13:23:09.000Z",
    "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
    "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18753.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66feb5c3d74232f132ea11ba",
      "avatarUrl": "/avatars/683bb03e904d85585cc5fe901b2f2d7c.svg",
      "fullname": "Xinyue Zeng",
      "name": "xyzeng2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17067",
      "authors": [
        {
          "_id": "6978940b026bdf0473117070",
          "user": {
            "_id": "6340333a733f9eef46913dc8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6340333a733f9eef46913dc8/BwhKgkqbuuxWsJv2s_JEH.png",
            "isPro": false,
            "fullname": "luozhou wang",
            "user": "wileewang",
            "type": "user"
          },
          "name": "Luozhou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:47.993Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117071",
          "user": {
            "_id": "634a91c53a0cd2d4985dc98a",
            "avatarUrl": "/avatars/fe680e683761c347cfa4bc9273b67b0d.svg",
            "isPro": false,
            "fullname": "ZhiFei Chen",
            "user": "zhifeichen097",
            "type": "user"
          },
          "name": "Zhifei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:53.764Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117072",
          "user": {
            "_id": "65214c46f6ceb915cc790275",
            "avatarUrl": "/avatars/c30541fd8ea55d479740f534a49e6248.svg",
            "isPro": false,
            "fullname": "Yihua Du",
            "user": "Duyh",
            "type": "user"
          },
          "name": "Yihua Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:59.764Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117073",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:30:59.380Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117074",
          "user": {
            "_id": "64758ee7d815855e4efa206b",
            "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
            "isPro": false,
            "fullname": "wenhang ge",
            "user": "spongy",
            "type": "user"
          },
          "name": "Wenhang Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:43:05.015Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117075",
          "user": {
            "_id": "65d5aa45dca2a85f0fe895f3",
            "avatarUrl": "/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg",
            "isPro": false,
            "fullname": "Guibao SHEN",
            "user": "PaulSHEN1",
            "type": "user"
          },
          "name": "Guibao Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:43:13.688Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117076",
          "user": {
            "_id": "64b4ab62eec33e27dcd733b5",
            "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
            "isPro": false,
            "fullname": "Xinli XU",
            "user": "Xxlbigbrother",
            "type": "user"
          },
          "name": "Xinli Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:43:19.655Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117077",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117078",
          "name": "Man Chen",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf0473117079",
          "user": {
            "_id": "65969bf8103a0e1e0b073125",
            "avatarUrl": "/avatars/408a5f6994741aaeddbc8d5a746c62f2.svg",
            "isPro": true,
            "fullname": "Tianshuo Xu",
            "user": "TSXu",
            "type": "user"
          },
          "name": "Tianshuo Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:43:32.909Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf047311707a",
          "user": {
            "_id": "64b74a45f902508f0d786505",
            "avatarUrl": "/avatars/8bc5aaa011642827e12524c4f0a56927.svg",
            "isPro": false,
            "fullname": "Peiran REN",
            "user": "lyraestar",
            "type": "user"
          },
          "name": "Peiran Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:43:38.782Z",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf047311707b",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf047311707c",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6978940b026bdf047311707d",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T19:00:18.000Z",
      "submittedOnDailyAt": "2026-01-27T08:02:03.483Z",
      "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
      "submittedOnDailyBy": {
        "_id": "634a91c53a0cd2d4985dc98a",
        "avatarUrl": "/avatars/fe680e683761c347cfa4bc9273b67b0d.svg",
        "isPro": false,
        "fullname": "ZhiFei Chen",
        "user": "zhifeichen097",
        "type": "user"
      },
      "summary": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.",
      "upvotes": 7,
      "discussionId": "6978940b026bdf047311707e",
      "githubRepo": "https://github.com/hit-perfect/Awesome-Video-World-Models",
      "githubRepoAddedBy": "user",
      "ai_summary": "Video generation models are categorized based on state construction and dynamics modeling approaches, with emphasis on transitioning evaluation metrics from visual quality to functional capabilities like physical persistence and causal reasoning.",
      "ai_keywords": [
        "video generation models",
        "world models",
        "state construction",
        "dynamics modeling",
        "implicit paradigms",
        "explicit paradigms",
        "context management",
        "latent compression",
        "knowledge integration",
        "architectural reformulation",
        "physical persistence",
        "causal reasoning",
        "data-driven memory",
        "compressed fidelity",
        "latent factor decoupling",
        "reasoning-prior integration"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "65ad19cac14c3cf579ad9b68",
        "name": "HKUSTGZ",
        "fullname": "HKUSTGZ"
      }
    },
    "publishedAt": "2026-01-22T14:00:18.000Z",
    "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
    "summary": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17067.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "634a91c53a0cd2d4985dc98a",
      "avatarUrl": "/avatars/fe680e683761c347cfa4bc9273b67b0d.svg",
      "fullname": "ZhiFei Chen",
      "name": "zhifeichen097",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65ad19cac14c3cf579ad9b68",
      "name": "HKUSTGZ",
      "fullname": "HKUSTGZ"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17958",
      "authors": [
        {
          "_id": "69789211026bdf047311705b",
          "name": "Ido Andrew Atad",
          "hidden": false
        },
        {
          "_id": "69789211026bdf047311705c",
          "user": {
            "_id": "65376feed325b3f02fb92c69",
            "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
            "isPro": false,
            "fullname": "Itamar Zimerman",
            "user": "ItamarZ",
            "type": "user"
          },
          "name": "Itamar Zimerman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:38:50.723Z",
          "hidden": false
        },
        {
          "_id": "69789211026bdf047311705d",
          "user": {
            "_id": "641051dcf52d7eb22e050f98",
            "avatarUrl": "/avatars/dcc2e51552b4443c0352a6632d8a1001.svg",
            "isPro": false,
            "fullname": "Shahar Katz",
            "user": "shaharkatz",
            "type": "user"
          },
          "name": "Shahar Katz",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:38:56.741Z",
          "hidden": false
        },
        {
          "_id": "69789211026bdf047311705e",
          "user": {
            "_id": "62f9f170363251ee40a08046",
            "avatarUrl": "/avatars/67ad459db44b605be0c1f6bd33363d6e.svg",
            "isPro": false,
            "fullname": "Lior Wolf",
            "user": "liorwolf",
            "type": "user"
          },
          "name": "Lior Wolf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:39:03.768Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/UPjVE_lk5W_MioVrm8oOa.png"
      ],
      "publishedAt": "2026-01-25T19:21:25.000Z",
      "submittedOnDailyAt": "2026-01-27T07:55:57.403Z",
      "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.",
      "upvotes": 1,
      "discussionId": "69789212026bdf047311705f",
      "ai_summary": "TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.",
      "ai_keywords": [
        "attention matrices",
        "transformers",
        "attention heads",
        "attention formulation",
        "attention-aggregation methods",
        "high-order attention-interaction tensor",
        "linear operator",
        "model interpretability",
        "model understanding",
        "residual connections",
        "feedforward networks",
        "normalization"
      ],
      "organization": {
        "_id": "6107dfc57602f8e9ed8bb5cb",
        "name": "tau",
        "fullname": "Tel Aviv University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628143727824-610b729f9da682cd54ad9adf.png"
      }
    },
    "publishedAt": "2026-01-25T14:21:25.000Z",
    "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
    "summary": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/UPjVE_lk5W_MioVrm8oOa.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17958.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6107dfc57602f8e9ed8bb5cb",
      "name": "tau",
      "fullname": "Tel Aviv University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1628143727824-610b729f9da682cd54ad9adf.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18137",
      "authors": [
        {
          "_id": "69784aee026bdf0473116f4e",
          "name": "Yinger Zhang",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f4f",
          "user": {
            "_id": "6746c6c99700a50f13a0eda9",
            "avatarUrl": "/avatars/8e8b82a9b73ff807d976fc48bb2e3edc.svg",
            "isPro": false,
            "fullname": "Shutong Jiang",
            "user": "Stjiang",
            "type": "user"
          },
          "name": "Shutong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T10:23:07.229Z",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f50",
          "user": {
            "_id": "64abc87cf79cb0c313821c11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64abc87cf79cb0c313821c11/tJa4iX5_f5XoSIzXt-rwF.jpeg",
            "isPro": false,
            "fullname": "Renhao Li",
            "user": "RioLee",
            "type": "user"
          },
          "name": "Renhao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T10:23:38.820Z",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f51",
          "user": {
            "_id": "654bead777401b47e6424f88",
            "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
            "isPro": false,
            "fullname": "Jianhong Tu",
            "user": "JianhongTu",
            "type": "user"
          },
          "name": "Jianhong Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T10:23:44.994Z",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f52",
          "name": "Yang Su",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f53",
          "name": "Lianghao Deng",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f54",
          "name": "Xudong Guo",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f55",
          "name": "Chenxu Lv",
          "hidden": false
        },
        {
          "_id": "69784aee026bdf0473116f56",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T10:24:12.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T04:43:49.000Z",
      "submittedOnDailyAt": "2026-01-27T07:52:17.482Z",
      "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": false,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.",
      "upvotes": 16,
      "discussionId": "69784aef026bdf0473116f57",
      "ai_summary": "DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.",
      "ai_keywords": [
        "agent evaluation",
        "long-horizon tasks",
        "global constrained optimization",
        "local constrained reasoning",
        "agentic LLMs",
        "explicit reasoning patterns",
        "parallel tool use"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-01-25T23:43:49.000Z",
    "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
    "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18137.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1213,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15860",
      "authors": [
        {
          "_id": "69772a475d41524304c13726",
          "user": {
            "_id": "66442866c989ff69972c0c1a",
            "avatarUrl": "/avatars/1104998a51e019d5247705dadea44188.svg",
            "isPro": false,
            "fullname": "Shui-Hsiang Hsu",
            "user": "adsl135789",
            "type": "user"
          },
          "name": "Shui-Hsiang Hsu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T16:12:55.294Z",
          "hidden": false
        },
        {
          "_id": "69772a475d41524304c13727",
          "user": {
            "_id": "672d96d0741fa214788392ef",
            "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
            "isPro": false,
            "fullname": "Chou, TSUNG-HSIANG",
            "user": "yumeow122",
            "type": "user"
          },
          "name": "Tsung-Hsiang Chou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:43:29.157Z",
          "hidden": false
        },
        {
          "_id": "69772a475d41524304c13728",
          "user": {
            "_id": "6320251d7196f93bc94ac4e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6320251d7196f93bc94ac4e3/DQyJNTyniltro3W2598ws.jpeg",
            "isPro": false,
            "fullname": "CHEN-JUI, YU",
            "user": "rui0828",
            "type": "user"
          },
          "name": "Chen-Jui Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T13:57:03.798Z",
          "hidden": false
        },
        {
          "_id": "69772a475d41524304c13729",
          "user": {
            "_id": "64b8a2408815d8652110388d",
            "avatarUrl": "/avatars/e973c8c047bc367ffa9c9e48f11493c5.svg",
            "isPro": false,
            "fullname": "Yao-Chung Fan",
            "user": "tomoto923",
            "type": "user"
          },
          "name": "Yao-Chung Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:03.738Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T11:08:46.000Z",
      "submittedOnDailyAt": "2026-01-27T07:48:22.015Z",
      "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
      "submittedOnDailyBy": {
        "_id": "672d96d0741fa214788392ef",
        "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
        "isPro": false,
        "fullname": "Chou, TSUNG-HSIANG",
        "user": "yumeow122",
        "type": "user"
      },
      "summary": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.",
      "upvotes": 9,
      "discussionId": "69772a475d41524304c1372a",
      "githubRepo": "https://github.com/adsl135789/STAR",
      "githubRepoAddedBy": "user",
      "ai_summary": "STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.",
      "ai_keywords": [
        "semantic clustering",
        "weighted fusion",
        "table retrieval",
        "semantic representation",
        "synthetic queries",
        "K-means clustering",
        "partial table",
        "embedding alignment"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "63468c63b21bd5e350c1d177",
        "name": "NchuNLPLab",
        "fullname": "NCHU NLP Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1665567811412-6346516ffe134dfd7a120e00.png"
      }
    },
    "publishedAt": "2026-01-22T06:08:46.000Z",
    "title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
    "summary": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15860.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "672d96d0741fa214788392ef",
      "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
      "fullname": "Chou, TSUNG-HSIANG",
      "name": "yumeow122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63468c63b21bd5e350c1d177",
      "name": "NchuNLPLab",
      "fullname": "NCHU NLP Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1665567811412-6346516ffe134dfd7a120e00.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.15015",
      "authors": [
        {
          "_id": "69777ba64c988cc0118d1d99",
          "user": {
            "_id": "682b01713e2301e26aff86ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cDNT2fo7ujfmzabB2AKzW.png",
            "isPro": false,
            "fullname": "Jannis Becktepe",
            "user": "becktepe",
            "type": "user"
          },
          "name": "Jannis Becktepe",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:43:19.480Z",
          "hidden": false
        },
        {
          "_id": "69777ba64c988cc0118d1d9a",
          "name": "Aleksandra Franz",
          "hidden": false
        },
        {
          "_id": "69777ba64c988cc0118d1d9b",
          "user": {
            "_id": "667ab4c3b3c1b9aac81388cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667ab4c3b3c1b9aac81388cc/G9uE6FaR8BZf1yoBvaApY.jpeg",
            "isPro": false,
            "fullname": "Nils Thuerey",
            "user": "thunil",
            "type": "user"
          },
          "name": "Nils Thuerey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:39:16.341Z",
          "hidden": false
        },
        {
          "_id": "69777ba64c988cc0118d1d9c",
          "name": "Sebastian Peitz",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-21T14:13:44.000Z",
      "submittedOnDailyAt": "2026-01-27T07:31:31.409Z",
      "title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
      "submittedOnDailyBy": {
        "_id": "682b01713e2301e26aff86ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cDNT2fo7ujfmzabB2AKzW.png",
        "isPro": false,
        "fullname": "Jannis Becktepe",
        "user": "becktepe",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.",
      "upvotes": 2,
      "discussionId": "69777ba74c988cc0118d1d9d",
      "githubRepo": "https://github.com/safe-autonomous-systems/fluidgym",
      "githubRepoAddedBy": "user",
      "ai_summary": "FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.",
      "ai_keywords": [
        "reinforcement learning",
        "active flow control",
        "PyTorch",
        "PICT solver",
        "differentiable benchmark",
        "PPO",
        "SAC"
      ],
      "githubStars": 21,
      "organization": {
        "_id": "690481754c36b01343dcc5be",
        "name": "safe-autonomous-systems",
        "fullname": "Safe Autonomous Systems @ TU Dortmund University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682b01713e2301e26aff86ca/MMLHDlziJNwQQHhxYd0ig.png"
      }
    },
    "publishedAt": "2026-01-21T09:13:44.000Z",
    "title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
    "summary": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15015.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "682b01713e2301e26aff86ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cDNT2fo7ujfmzabB2AKzW.png",
      "fullname": "Jannis Becktepe",
      "name": "becktepe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "690481754c36b01343dcc5be",
      "name": "safe-autonomous-systems",
      "fullname": "Safe Autonomous Systems @ TU Dortmund University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682b01713e2301e26aff86ca/MMLHDlziJNwQQHhxYd0ig.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.12042",
      "authors": [
        {
          "_id": "69788a15026bdf0473117038",
          "name": "Xiaomei Zhang",
          "hidden": false
        },
        {
          "_id": "69788a15026bdf0473117039",
          "user": {
            "_id": "66930fd3a6b36c9e7852e96c",
            "avatarUrl": "/avatars/78263e1edb13773941489b04bfd74aba.svg",
            "isPro": false,
            "fullname": "zhang",
            "user": "plll123",
            "type": "user"
          },
          "name": "Zhaoxi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T13:56:55.009Z",
          "hidden": false
        },
        {
          "_id": "69788a15026bdf047311703a",
          "name": "Leo Yu Zhang",
          "hidden": false
        },
        {
          "_id": "69788a15026bdf047311703b",
          "name": "Yanjun Zhang",
          "hidden": false
        },
        {
          "_id": "69788a15026bdf047311703c",
          "name": "Guanhong Tao",
          "hidden": false
        },
        {
          "_id": "69788a15026bdf047311703d",
          "name": "Shirui Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-17T13:02:41.000Z",
      "submittedOnDailyAt": "2026-01-27T07:22:01.765Z",
      "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "66930fd3a6b36c9e7852e96c",
        "avatarUrl": "/avatars/78263e1edb13773941489b04bfd74aba.svg",
        "isPro": false,
        "fullname": "zhang",
        "user": "plll123",
        "type": "user"
      },
      "summary": "Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.",
      "upvotes": 2,
      "discussionId": "69788a15026bdf047311703e",
      "ai_summary": "Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.",
      "ai_keywords": [
        "visual token compression",
        "Large Vision-Language Models",
        "robustness",
        "token importance ranking",
        "compression mechanism",
        "Compression-Aware Attack",
        "black-box settings",
        "Transfer CAA",
        "defense mechanisms"
      ]
    },
    "publishedAt": "2026-01-17T08:02:41.000Z",
    "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
    "summary": "Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12042.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66930fd3a6b36c9e7852e96c",
      "avatarUrl": "/avatars/78263e1edb13773941489b04bfd74aba.svg",
      "fullname": "zhang",
      "name": "plll123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17640",
      "authors": [
        {
          "_id": "697884d6026bdf0473117021",
          "name": "Anfeng Xu",
          "hidden": false
        },
        {
          "_id": "697884d6026bdf0473117022",
          "user": {
            "_id": "64092a1ab6a334f53e278b3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg",
            "isPro": false,
            "fullname": "Tiantian Feng",
            "user": "tiantiaf",
            "type": "user"
          },
          "name": "Tiantian Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:34:30.149Z",
          "hidden": false
        },
        {
          "_id": "697884d6026bdf0473117023",
          "name": "Somer Bishop",
          "hidden": false
        },
        {
          "_id": "697884d6026bdf0473117024",
          "name": "Catherine Lord",
          "hidden": false
        },
        {
          "_id": "697884d6026bdf0473117025",
          "name": "Shrikanth Narayanan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-25T01:00:41.000Z",
      "submittedOnDailyAt": "2026-01-27T06:58:41.225Z",
      "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
      "submittedOnDailyBy": {
        "_id": "64092a1ab6a334f53e278b3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg",
        "isPro": false,
        "fullname": "Tiantian Feng",
        "user": "tiantiaf",
        "type": "user"
      },
      "summary": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available",
      "upvotes": 5,
      "discussionId": "697884d6026bdf0473117026",
      "githubRepo": "https://github.com/usc-sail/joint-asr-diarization-child-adult",
      "githubRepoAddedBy": "user",
      "ai_summary": "A unified end-to-end framework extends the Whisper architecture to jointly model automatic speech recognition and speaker diarization for child-adult interactions, improving transcription accuracy and scalability.",
      "ai_keywords": [
        "Whisper encoder-decoder architecture",
        "end-to-end framework",
        "speaker diarization",
        "speech recognition",
        "serialized output training",
        "frame-level diarization head",
        "diarization-guided silence suppression",
        "state-machine-based forced decoding",
        "multi-talker word error rates",
        "speaker-attributed transcripts"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-01-24T20:00:41.000Z",
    "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
    "summary": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17640.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64092a1ab6a334f53e278b3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg",
      "fullname": "Tiantian Feng",
      "name": "tiantiaf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17277",
      "authors": [
        {
          "_id": "6978819c026bdf047311700e",
          "user": {
            "_id": "65af40481216d503271e7d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af40481216d503271e7d95/7BvAFiSHbL4rShMzdsLyd.png",
            "isPro": false,
            "fullname": "Mohammad Rifqi Farhansyah",
            "user": "rifqifarhansyah",
            "type": "user"
          },
          "name": "Mohammad Rifqi Farhansyah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:42:31.957Z",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf047311700f",
          "user": {
            "_id": "64e44caa3ad886e1af2768f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e44caa3ad886e1af2768f5/PUjbVLCspXNvzRMfmyZXR.jpeg",
            "isPro": false,
            "fullname": "Hanif Muhammad Zhafran",
            "user": "hanifmz0711",
            "type": "user"
          },
          "name": "Hanif Muhammad Zhafran",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:39:29.608Z",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117010",
          "user": {
            "_id": "60c7fe961ef9b0a2ad1d398e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c7fe961ef9b0a2ad1d398e/S4ewjcFp_q0Fj6QK8GwtL.jpeg",
            "isPro": false,
            "fullname": "Farid Adilazuarda",
            "user": "faridlazuarda",
            "type": "user"
          },
          "name": "Farid Adilazuarda",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:39:35.653Z",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117011",
          "name": "Shamsuddeen Hassan Muhammad",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117012",
          "name": "Maryam Ibrahim Mukhtar",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117013",
          "name": "Nedjma Ousidhoum",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117014",
          "user": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "isPro": false,
            "fullname": "Genta Indra Winata",
            "user": "gentaiscool",
            "type": "user"
          },
          "name": "Genta Indra Winata",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:39:54.342Z",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117015",
          "name": "Ayu Purwarianti",
          "hidden": false
        },
        {
          "_id": "6978819c026bdf0473117016",
          "user": {
            "_id": "61a4dc053205e107691e0d82",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a4dc053205e107691e0d82/BESoEHlHYXstXudh6dOdT.jpeg",
            "isPro": true,
            "fullname": "Alham Fikri Aji",
            "user": "afaji",
            "type": "user"
          },
          "name": "Alham Fikri Aji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:40:03.188Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-24T03:31:08.000Z",
      "submittedOnDailyAt": "2026-01-27T06:46:04.281Z",
      "title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
      "submittedOnDailyBy": {
        "_id": "65af40481216d503271e7d95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af40481216d503271e7d95/7BvAFiSHbL4rShMzdsLyd.png",
        "isPro": false,
        "fullname": "Mohammad Rifqi Farhansyah",
        "user": "rifqifarhansyah",
        "type": "user"
      },
      "summary": "Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.",
      "upvotes": 5,
      "discussionId": "6978819c026bdf0473117017",
      "ai_summary": "Code-switching presents complex challenges in multilingual communication that current language models struggle to address effectively.",
      "ai_keywords": [
        "code-switching",
        "multilingual discourse",
        "natural language processing",
        "language models",
        "dialogue understanding"
      ]
    },
    "publishedAt": "2026-01-23T22:31:08.000Z",
    "title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
    "summary": "Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17277.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65af40481216d503271e7d95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af40481216d503271e7d95/7BvAFiSHbL4rShMzdsLyd.png",
      "fullname": "Mohammad Rifqi Farhansyah",
      "name": "rifqifarhansyah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.14127",
      "authors": [
        {
          "_id": "69721234c1c7409747bf9799",
          "user": {
            "_id": "66f91b549e8e332d731f2e7a",
            "avatarUrl": "/avatars/4bfe8b0a1ece93d3c745a0b288d8c55f.svg",
            "isPro": false,
            "fullname": "Renmiao Chen",
            "user": "smilestruggler",
            "type": "user"
          },
          "name": "Renmiao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-23T09:38:50.162Z",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979a",
          "name": "Yida Lu",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979b",
          "user": {
            "_id": "6617a9c192abaae4ecd5c066",
            "avatarUrl": "/avatars/64994d014fac7f84e2925e568cc047f8.svg",
            "isPro": false,
            "fullname": "Shiyao Cui",
            "user": "ShiyaoCui",
            "type": "user"
          },
          "name": "Shiyao Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:34.438Z",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979c",
          "user": {
            "_id": "67e617d4470f96a302734e16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png",
            "isPro": false,
            "fullname": "Xuan Ouyang",
            "user": "YoungXuan",
            "type": "user"
          },
          "name": "Xuan Ouyang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:11.991Z",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979d",
          "user": {
            "_id": "65b5d65b625ac670a79b52a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b5d65b625ac670a79b52a8/aBPXeSgh-_HQQWuPN89OK.jpeg",
            "isPro": false,
            "fullname": "Victor Shea-Jay Huang",
            "user": "jeix",
            "type": "user"
          },
          "name": "Victor Shea-Jay Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:06.787Z",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979e",
          "user": {
            "_id": "664694c9b6c9eb0638bd27a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dfDxBdN9zOITbGyumGJ8z.png",
            "isPro": false,
            "fullname": "Shumin Zhang",
            "user": "shuminCA",
            "type": "user"
          },
          "name": "Shumin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:42:17.270Z",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf979f",
          "name": "Chengwei Pan",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf97a0",
          "name": "Han Qiu",
          "hidden": false
        },
        {
          "_id": "69721234c1c7409747bf97a1",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-20T16:24:18.000Z",
      "submittedOnDailyAt": "2026-01-27T05:49:30.130Z",
      "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
      "submittedOnDailyBy": {
        "_id": "66f91b549e8e332d731f2e7a",
        "avatarUrl": "/avatars/4bfe8b0a1ece93d3c745a0b288d8c55f.svg",
        "isPro": false,
        "fullname": "Renmiao Chen",
        "user": "smilestruggler",
        "type": "user"
      },
      "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
      "upvotes": 2,
      "discussionId": "69721235c1c7409747bf97a2",
      "githubRepo": "https://github.com/thu-coai/MIR-SafetyBench",
      "githubRepoAddedBy": "user",
      "ai_summary": "Multi-image reasoning safety benchmark reveals that advanced multimodal models exhibit increased vulnerability and superficial safety responses, with unsafe generations showing lower attention entropy.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "multi-image reasoning",
        "safety benchmark",
        "attention entropy"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "5fa0fb41a13e063b8b2b5d1f",
        "name": "thu-coai",
        "fullname": "Conversational AI (CoAI) group from Tsinghua University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1637728594024-60c960d730ae5cb2dd246a92.png"
      }
    },
    "publishedAt": "2026-01-20T11:24:18.000Z",
    "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
    "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14127.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66f91b549e8e332d731f2e7a",
      "avatarUrl": "/avatars/4bfe8b0a1ece93d3c745a0b288d8c55f.svg",
      "fullname": "Renmiao Chen",
      "name": "smilestruggler",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5fa0fb41a13e063b8b2b5d1f",
      "name": "thu-coai",
      "fullname": "Conversational AI (CoAI) group from Tsinghua University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1637728594024-60c960d730ae5cb2dd246a92.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18217",
      "authors": [
        {
          "_id": "697857f4026bdf0473116f7e",
          "name": "Zhihan Liu",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f7f",
          "name": "Lin Guan",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f80",
          "name": "Yixin Nie",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f81",
          "user": {
            "_id": "63e0a50242591dda0b9dca5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
            "isPro": false,
            "fullname": "Kai Zhang",
            "user": "drogozhang",
            "type": "user"
          },
          "name": "Kai Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T08:31:49.126Z",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f82",
          "name": "Zhuoqun Hao",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f83",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f84",
          "name": "Asli Celikyilmaz",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f85",
          "name": "Zhaoran Wang",
          "hidden": false
        },
        {
          "_id": "697857f4026bdf0473116f86",
          "name": "Na Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T07:07:03.000Z",
      "submittedOnDailyAt": "2026-01-27T04:19:12.826Z",
      "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
      "submittedOnDailyBy": {
        "_id": "68fbafe8677904dd08bc9970",
        "avatarUrl": "/avatars/1cf6ed148a5bac8df423f2f81277610d.svg",
        "isPro": true,
        "fullname": "Lin G",
        "user": "lgmeta",
        "type": "user"
      },
      "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.",
      "upvotes": 8,
      "discussionId": "697857f4026bdf0473116f87",
      "ai_summary": "Research examines factors influencing out-of-domain performance in reinforcement learning agents, identifying state information richness and planning complexity as key determinants, while proposing a randomization technique to enhance cross-domain robustness.",
      "ai_keywords": [
        "reinforcement learning",
        "out-of-domain performance",
        "state information richness",
        "planning complexity",
        "goal reachability",
        "trajectory length",
        "catastrophic forgetting",
        "step-by-step thinking",
        "randomization technique"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2026-01-26T02:07:03.000Z",
    "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
    "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18217.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "68fbafe8677904dd08bc9970",
      "avatarUrl": "/avatars/1cf6ed148a5bac8df423f2f81277610d.svg",
      "fullname": "Lin G",
      "name": "lgmeta",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.13599",
      "authors": [
        {
          "_id": "69785d8f026bdf0473116f8d",
          "name": "Linrui Ma",
          "hidden": false
        },
        {
          "_id": "69785d8f026bdf0473116f8e",
          "name": "Yufei Cui",
          "hidden": false
        },
        {
          "_id": "69785d8f026bdf0473116f8f",
          "user": {
            "_id": "65e52e7d27dc8aa470a640e3",
            "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg",
            "isPro": false,
            "fullname": "hankai",
            "user": "hankaixyz",
            "type": "user"
          },
          "name": "Kai Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:38:15.388Z",
          "hidden": false
        },
        {
          "_id": "69785d8f026bdf0473116f90",
          "name": "Yunhe Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-20T05:00:26.000Z",
      "submittedOnDailyAt": "2026-01-27T04:10:00.100Z",
      "title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion",
      "submittedOnDailyBy": {
        "_id": "65e52e7d27dc8aa470a640e3",
        "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg",
        "isPro": false,
        "fullname": "hankai",
        "user": "hankaixyz",
        "type": "user"
      },
      "summary": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a 'draft-then-refine' framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.",
      "upvotes": 4,
      "discussionId": "69785d90026bdf0473116f91",
      "projectPage": "https://noah-dllm.github.io/",
      "ai_summary": "A 'draft-then-refine' framework for discrete diffusion language models that recovers global contextual understanding while maintaining semi-autoregressive efficiency through block diffusion and global bidirectional processing.",
      "ai_keywords": [
        "discrete diffusion language models",
        "global bidirectional contextual capability",
        "block-based diffusion",
        "autoregressive priors",
        "semi-autoregressive paradigm",
        "diffusion in diffusion",
        "draft-then-refine framework",
        "irreversibility",
        "myopia",
        "snapshot confidence remasking",
        "mix-scale training",
        "generative perplexity"
      ],
      "organization": {
        "_id": "5f83c275f0801648bf88454a",
        "name": "huawei-noah",
        "fullname": "HUAWEI Noah's Ark Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
      }
    },
    "publishedAt": "2026-01-20T00:00:26.000Z",
    "title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion",
    "summary": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a 'draft-then-refine' framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13599.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "65e52e7d27dc8aa470a640e3",
      "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg",
      "fullname": "hankai",
      "name": "hankaixyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5f83c275f0801648bf88454a",
      "name": "huawei-noah",
      "fullname": "HUAWEI Noah's Ark Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18778",
      "authors": [
        {
          "_id": "69783108026bdf0473116e3c",
          "user": {
            "_id": "6328ab511558dac67c45af92",
            "avatarUrl": "/avatars/1134657afe749b782f89fcabe960b774.svg",
            "isPro": false,
            "fullname": "Shobhita Sundaram",
            "user": "ssundaram",
            "type": "user"
          },
          "name": "Shobhita Sundaram",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:27.764Z",
          "hidden": false
        },
        {
          "_id": "69783108026bdf0473116e3d",
          "name": "John Quan",
          "hidden": false
        },
        {
          "_id": "69783108026bdf0473116e3e",
          "name": "Ariel Kwiatkowski",
          "hidden": false
        },
        {
          "_id": "69783108026bdf0473116e3f",
          "name": "Kartik Ahuja",
          "hidden": false
        },
        {
          "_id": "69783108026bdf0473116e40",
          "name": "Yann Ollivier",
          "hidden": false
        },
        {
          "_id": "69783108026bdf0473116e41",
          "user": {
            "_id": "65ce30e06da01df536eded5a",
            "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg",
            "isPro": false,
            "fullname": "Julia Kempe",
            "user": "Knykny",
            "type": "user"
          },
          "name": "Julia Kempe",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:24.999Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T18:46:56.000Z",
      "submittedOnDailyAt": "2026-01-27T03:59:09.071Z",
      "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
      "submittedOnDailyBy": {
        "_id": "65ce30e06da01df536eded5a",
        "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg",
        "isPro": false,
        "fullname": "Julia Kempe",
        "user": "Knykny",
        "type": "user"
      },
      "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
      "upvotes": 25,
      "discussionId": "69783109026bdf0473116e42",
      "ai_summary": "A self-improvement framework enables pretrained language models to generate automated curricula for solving previously unsolvable problems by leveraging latent knowledge and meta-reinforcement learning.",
      "ai_keywords": [
        "pretrained LLM",
        "reinforcement learning",
        "finetuning",
        "meta-RL",
        "automated curriculum",
        "self-improvement framework",
        "teacher-student model",
        "binary rewards",
        "sparse rewards",
        "latent capacity",
        "stepping stones",
        "structural quality",
        "well-posedness"
      ],
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2026-01-26T13:46:56.000Z",
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18778.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65ce30e06da01df536eded5a",
      "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg",
      "fullname": "Julia Kempe",
      "name": "Knykny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18418",
      "authors": [
        {
          "_id": "69785315026bdf0473116f6a",
          "user": {
            "_id": "62dce08bb2c60f29c3d0a5da",
            "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg",
            "isPro": false,
            "fullname": "Ji Zeng",
            "user": "stargazerzj",
            "type": "user"
          },
          "name": "Ji Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T08:31:51.245Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f6b",
          "name": "Dayuan Fu",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f6c",
          "name": "Tiantian Mi",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f6d",
          "user": {
            "_id": "6651f8441b1ce9f4a61a09ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YM8m6_FLN_eF5PmqD7BfA.png",
            "isPro": false,
            "fullname": "Zhuang Yumin",
            "user": "Astricaelus",
            "type": "user"
          },
          "name": "Yumin Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:31:06.285Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f6e",
          "user": {
            "_id": "6865e6b362fc5689c5e67733",
            "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg",
            "isPro": false,
            "fullname": "Yaxing Huang",
            "user": "Rookie-Noob-Newbie",
            "type": "user"
          },
          "name": "Yaxing Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:00:30.220Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f6f",
          "user": {
            "_id": "67638cc0d63e4b348e8a5fa3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png",
            "isPro": false,
            "fullname": "Xuefeng Li",
            "user": "drxuefeng",
            "type": "user"
          },
          "name": "Xuefeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:00:37.248Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f70",
          "name": "Lyumanshan Ye",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f71",
          "name": "Muhang Xie",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f72",
          "name": "Qishuo Hua",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f73",
          "name": "Zhen Huang",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f74",
          "user": {
            "_id": "66d01e4401f2a6b4cd93ad87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
            "isPro": false,
            "fullname": "Mohan Jiang (SII)",
            "user": "mhjiang0408",
            "type": "user"
          },
          "name": "Mohan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:00:23.438Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f75",
          "name": "Hanning Wang",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f76",
          "user": {
            "_id": "66fa544c54f87b607fbffd2e",
            "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg",
            "isPro": false,
            "fullname": "Jifan Lin",
            "user": "evanlin2570",
            "type": "user"
          },
          "name": "Jifan Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:00:57.029Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f77",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6002c316698168af3bb9f4a6/M2J2QFCRc5RhYRoz7OuZN.png",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:31:08.388Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f78",
          "name": "Jie Sun",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f79",
          "user": {
            "_id": "684faf712acd915b5afc055f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg",
            "isPro": false,
            "fullname": "Yunze Wu",
            "user": "wyzmike",
            "type": "user"
          },
          "name": "Yunze Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:01:06.063Z",
          "hidden": false
        },
        {
          "_id": "69785315026bdf0473116f7a",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T12:20:18.000Z",
      "submittedOnDailyAt": "2026-01-27T03:34:37.777Z",
      "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
      "submittedOnDailyBy": {
        "_id": "66d01e4401f2a6b4cd93ad87",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
        "isPro": false,
        "fullname": "Mohan Jiang (SII)",
        "user": "mhjiang0408",
        "type": "user"
      },
      "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...",
      "upvotes": 113,
      "discussionId": "69785315026bdf0473116f7b",
      "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev",
      "githubRepoAddedBy": "user",
      "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.",
      "ai_keywords": [
        "Large Language Model",
        "agentic software engineering",
        "mid-training",
        "distribution mismatch",
        "agent-native data",
        "contextually-native trajectories",
        "environmentally-native trajectories",
        "SWE-Bench Verified",
        "Kimi-Dev",
        "resolution rates"
      ],
      "githubStars": 31,
      "organization": {
        "_id": "630bc2d186b8b9904c33ce1b",
        "name": "GAIR",
        "fullname": "SII - GAIR",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"
      }
    },
    "publishedAt": "2026-01-26T07:20:18.000Z",
    "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
    "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "66d01e4401f2a6b4cd93ad87",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png",
      "fullname": "Mohan Jiang (SII)",
      "name": "mhjiang0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "630bc2d186b8b9904c33ce1b",
      "name": "GAIR",
      "fullname": "SII - GAIR",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18731",
      "authors": [
        {
          "_id": "69782efc026bdf0473116e26",
          "user": {
            "_id": "653a111eee5888edef9182cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a111eee5888edef9182cf/7jQ08JDk2UEBla91At8QR.jpeg",
            "isPro": false,
            "fullname": "Hongru Cai",
            "user": "HongruCai",
            "type": "user"
          },
          "name": "Hongru Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:42:36.597Z",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e27",
          "name": "Yongqi Li",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e28",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e29",
          "name": "Fengbin Zhu",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e2a",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e2b",
          "name": "Fuli Feng",
          "hidden": false
        },
        {
          "_id": "69782efc026bdf0473116e2c",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T17:55:52.000Z",
      "submittedOnDailyAt": "2026-01-27T03:21:14.277Z",
      "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
      "submittedOnDailyBy": {
        "_id": "653a111eee5888edef9182cf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a111eee5888edef9182cf/7jQ08JDk2UEBla91At8QR.jpeg",
        "isPro": false,
        "fullname": "Hongru Cai",
        "user": "HongruCai",
        "type": "user"
      },
      "summary": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
      "upvotes": 6,
      "discussionId": "69782efc026bdf0473116e2d",
      "githubRepo": "https://github.com/ModalityDance/MRM",
      "githubRepoAddedBy": "user",
      "ai_summary": "Meta Reward Modeling reformulates personalized reward modeling as a meta-learning problem to enable efficient adaptation to individual users with limited feedback.",
      "ai_keywords": [
        "meta-learning",
        "reward modeling",
        "personalized reward models",
        "Model-Agnostic Meta-Learning",
        "MAML",
        "few-shot personalization",
        "Robust Personalization Objective",
        "RPO"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "69396d0f6ef210a3d45ac4b7",
        "name": "ModalityDance",
        "fullname": "ModalityDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/653a111eee5888edef9182cf/7BPn5_PnfH27PkAaLQnxW.png"
      }
    },
    "publishedAt": "2026-01-26T12:55:52.000Z",
    "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
    "summary": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18731.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "653a111eee5888edef9182cf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a111eee5888edef9182cf/7jQ08JDk2UEBla91At8QR.jpeg",
      "fullname": "Hongru Cai",
      "name": "HongruCai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69396d0f6ef210a3d45ac4b7",
      "name": "ModalityDance",
      "fullname": "ModalityDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/653a111eee5888edef9182cf/7BPn5_PnfH27PkAaLQnxW.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18157",
      "authors": [
        {
          "_id": "69784151026bdf0473116f3b",
          "name": "Aniket Rege",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f3c",
          "name": "Arka Sadhu",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f3d",
          "name": "Yuliang Li",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f3e",
          "name": "Kejie Li",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f3f",
          "name": "Ramya Korlakai Vinayak",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f40",
          "name": "Yuning Chai",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f41",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "69784151026bdf0473116f42",
          "name": "Hyo Jin Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T05:20:47.000Z",
      "submittedOnDailyAt": "2026-01-27T02:08:55.039Z",
      "title": "Agentic Very Long Video Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.",
      "upvotes": 7,
      "discussionId": "69784151026bdf0473116f43",
      "ai_summary": "An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.",
      "ai_keywords": [
        "entity scene graphs",
        "agentic framework",
        "long-horizon video understanding",
        "structured search",
        "temporal reasoning",
        "cross-modal reasoning",
        "EgoLifeQA",
        "Video-MME"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2026-01-26T00:20:47.000Z",
    "title": "Agentic Very Long Video Understanding",
    "summary": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18157.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 218,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18759",
      "authors": [
        {
          "_id": "69784102026bdf0473116f34",
          "name": "Junling Wang",
          "hidden": false
        },
        {
          "_id": "69784102026bdf0473116f35",
          "user": {
            "_id": "67ec110027088e548d5f3cb1",
            "avatarUrl": "/avatars/5a26a99b214877035528ab0d33febed4.svg",
            "isPro": false,
            "fullname": "Hongyi Lan",
            "user": "HenryLhy",
            "type": "user"
          },
          "name": "Hongyi Lan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:41:41.357Z",
          "hidden": false
        },
        {
          "_id": "69784102026bdf0473116f36",
          "name": "Xiaotian Su",
          "hidden": false
        },
        {
          "_id": "69784102026bdf0473116f37",
          "name": "Mustafa Doga Dogan",
          "hidden": false
        },
        {
          "_id": "69784102026bdf0473116f38",
          "name": "April Yi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T18:26:49.000Z",
      "submittedOnDailyAt": "2026-01-27T02:07:37.986Z",
      "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration.",
      "upvotes": 2,
      "discussionId": "69784102026bdf0473116f39",
      "ai_summary": "UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.",
      "ai_keywords": [
        "multimodal retrieval-augmented generation",
        "example-driven design",
        "iterative search",
        "design adaptation",
        "source transparency cues"
      ]
    },
    "publishedAt": "2026-01-26T13:26:49.000Z",
    "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
    "summary": "Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18759.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 218,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.17124",
      "authors": [
        {
          "_id": "6978407f026bdf0473116f26",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f27",
          "user": {
            "_id": "646df3c04ad7f907279f14c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
            "isPro": false,
            "fullname": "Zongjian Li",
            "user": "chestnutlzj",
            "type": "user"
          },
          "name": "Zongjian Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T14:27:26.565Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f28",
          "user": {
            "_id": "66915a572c1a3a8edcc977b4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66915a572c1a3a8edcc977b4/2tANTgj48VQMgCcEcdkwE.jpeg",
            "isPro": false,
            "fullname": "Yuwei Niu",
            "user": "Yuwei-Niu",
            "type": "user"
          },
          "name": "Yuwei Niu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:08.456Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f29",
          "user": {
            "_id": "642e427f6748dd4f8eeb2f38",
            "avatarUrl": "/avatars/07158ff6aa1803c846403594c5d55a34.svg",
            "isPro": false,
            "fullname": "Kaixiong Gong",
            "user": "kxgong",
            "type": "user"
          },
          "name": "Kaixiong Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:14.260Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2a",
          "user": {
            "_id": "646de6402fd5a8eb8c518aa6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646de6402fd5a8eb8c518aa6/HYWb8-fT1kTm-ROBr1-0X.jpeg",
            "isPro": false,
            "fullname": "yunyangge",
            "user": "yunyangge",
            "type": "user"
          },
          "name": "Yunyang Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:20.323Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2b",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2c",
          "user": {
            "_id": "62f0c4abe2999b231e5a893c",
            "avatarUrl": "/avatars/90da268b877a7ffe6665075c84018a83.svg",
            "isPro": false,
            "fullname": "Mingzhe Zheng",
            "user": "Dunge0nMaster",
            "type": "user"
          },
          "name": "Mingzhe Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:29.309Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2d",
          "name": "JianWei Zhang",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2e",
          "user": {
            "_id": "680d79e978e732e7141e9280",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YzX3HGEEHCXDcFymaiREA.png",
            "isPro": false,
            "fullname": "Miles Yang",
            "user": "dovahkiin648",
            "type": "user"
          },
          "name": "Miles Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:41.196Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f2f",
          "name": "Zhao Zhong",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f30",
          "user": {
            "_id": "63d0cc736b985b0f25d0412c",
            "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg",
            "isPro": false,
            "fullname": "Bo",
            "user": "Liefeng",
            "type": "user"
          },
          "name": "Liefeng Bo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:28:53.581Z",
          "hidden": false
        },
        {
          "_id": "6978407f026bdf0473116f31",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/xWMb_HtZh0Ud4FGpmfnyj.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/gVa2qUTv-eqFCiHr_gHUe.png"
      ],
      "publishedAt": "2026-01-23T19:00:35.000Z",
      "submittedOnDailyAt": "2026-01-27T02:06:14.421Z",
      "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
      "submittedOnDailyBy": {
        "_id": "6367a8175bb06007ea099b8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
        "isPro": false,
        "fullname": "linbin",
        "user": "LanguageBind",
        "type": "user"
      },
      "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ",
      "upvotes": 27,
      "discussionId": "6978407f026bdf0473116f32",
      "githubRepo": "https://github.com/Tencent-Hunyuan/iFSQ",
      "githubRepoAddedBy": "user",
      "ai_summary": "Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "VQ-VAEs",
        "VAEs",
        "finite scalar quantization",
        "activation collapse",
        "distribution-matching mapping",
        "iFSQ",
        "representation alignment",
        "LlamaGen-REPA"
      ],
      "githubStars": 66,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2026-01-23T14:00:35.000Z",
    "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
    "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/xWMb_HtZh0Ud4FGpmfnyj.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/gVa2qUTv-eqFCiHr_gHUe.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17124.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6367a8175bb06007ea099b8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
      "fullname": "linbin",
      "name": "LanguageBind",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 198,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18130",
      "authors": [
        {
          "_id": "697835b2026bdf0473116eb8",
          "user": {
            "_id": "66323843a6ef9d3afdf11fbb",
            "avatarUrl": "/avatars/2fe9313634fa30dfa3ca36de5cd094f0.svg",
            "isPro": false,
            "fullname": "Jize",
            "user": "Jize1",
            "type": "user"
          },
          "name": "Jize Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:40:22.251Z",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116eb9",
          "name": "Han Wu",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116eba",
          "user": {
            "_id": "64c8f16736c11430f31d9bf6",
            "avatarUrl": "/avatars/6907df1fedbcedc63046e7457a6b5950.svg",
            "isPro": false,
            "fullname": "zhiyuanyou",
            "user": "zhiyuanyou",
            "type": "user"
          },
          "name": "Zhiyuan You",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:40:28.630Z",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ebb",
          "name": "Yiming Song",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ebc",
          "name": "Yijun Wang",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ebd",
          "name": "Zifei Shan",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ebe",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ebf",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ec0",
          "user": {
            "_id": "64ba8a11c0f19c90256c5af6",
            "avatarUrl": "/avatars/e7f8dde9a1b7d89328af662a465b7441.svg",
            "isPro": false,
            "fullname": "Xinyi Le",
            "user": "larryle",
            "type": "user"
          },
          "name": "Xinyi Le",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:40:47.725Z",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ec1",
          "name": "Cailian Chen",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ec2",
          "name": "Xinping Guan",
          "hidden": false
        },
        {
          "_id": "697835b2026bdf0473116ec3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T04:22:22.000Z",
      "submittedOnDailyAt": "2026-01-27T01:55:15.937Z",
      "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
      "submittedOnDailyBy": {
        "_id": "66323843a6ef9d3afdf11fbb",
        "avatarUrl": "/avatars/2fe9313634fa30dfa3ca36de5cd094f0.svg",
        "isPro": false,
        "fullname": "Jize",
        "user": "Jize1",
        "type": "user"
      },
      "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.",
      "upvotes": 1,
      "discussionId": "697835b2026bdf0473116ec4",
      "ai_summary": "RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.",
      "ai_keywords": [
        "mixture-of-agents",
        "LLM judges",
        "dynamic routing",
        "lightweight scorer",
        "model ranking mechanism",
        "coarse-grained performance",
        "posterior correction",
        "inference efficiency"
      ]
    },
    "publishedAt": "2026-01-25T23:22:22.000Z",
    "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
    "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18130.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66323843a6ef9d3afdf11fbb",
      "avatarUrl": "/avatars/2fe9313634fa30dfa3ca36de5cd094f0.svg",
      "fullname": "Jize",
      "name": "Jize1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.15849",
      "authors": [
        {
          "_id": "69772a575d41524304c1372c",
          "user": {
            "_id": "672d96d0741fa214788392ef",
            "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
            "isPro": false,
            "fullname": "Chou, TSUNG-HSIANG",
            "user": "yumeow122",
            "type": "user"
          },
          "name": "Tsung-Hsiang Chou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-26T09:08:35.394Z",
          "hidden": false
        },
        {
          "_id": "69772a575d41524304c1372d",
          "user": {
            "_id": "6320251d7196f93bc94ac4e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6320251d7196f93bc94ac4e3/DQyJNTyniltro3W2598ws.jpeg",
            "isPro": false,
            "fullname": "CHEN-JUI, YU",
            "user": "rui0828",
            "type": "user"
          },
          "name": "Chen-Jui Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T13:57:01.119Z",
          "hidden": false
        },
        {
          "_id": "69772a575d41524304c1372e",
          "user": {
            "_id": "66442866c989ff69972c0c1a",
            "avatarUrl": "/avatars/1104998a51e019d5247705dadea44188.svg",
            "isPro": false,
            "fullname": "Shui-Hsiang Hsu",
            "user": "adsl135789",
            "type": "user"
          },
          "name": "Shui-Hsiang Hsu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:31:41.735Z",
          "hidden": false
        },
        {
          "_id": "69772a575d41524304c1372f",
          "user": {
            "_id": "64b8a2408815d8652110388d",
            "avatarUrl": "/avatars/e973c8c047bc367ffa9c9e48f11493c5.svg",
            "isPro": false,
            "fullname": "Yao-Chung Fan",
            "user": "tomoto923",
            "type": "user"
          },
          "name": "Yao-Chung Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:31:49.755Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T10:58:56.000Z",
      "submittedOnDailyAt": "2026-01-27T01:45:17.937Z",
      "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
      "submittedOnDailyBy": {
        "_id": "672d96d0741fa214788392ef",
        "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
        "isPro": false,
        "fullname": "Chou, TSUNG-HSIANG",
        "user": "yumeow122",
        "type": "user"
      },
      "summary": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.",
      "upvotes": 12,
      "discussionId": "69772a585d41524304c13730",
      "githubRepo": "https://github.com/yumeow0122/CGPT",
      "githubRepoAddedBy": "user",
      "ai_summary": "CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.",
      "ai_keywords": [
        "embedding models",
        "table retrieval",
        "LLM-based retrieval augmentation",
        "synthetic queries",
        "hard-negative contrastive fine-tuning",
        "K-means clustering",
        "semantic compression",
        "query-table mismatch",
        "cross-domain generalization"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "63468c63b21bd5e350c1d177",
        "name": "NchuNLPLab",
        "fullname": "NCHU NLP Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1665567811412-6346516ffe134dfd7a120e00.png"
      }
    },
    "publishedAt": "2026-01-22T05:58:56.000Z",
    "title": "CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval",
    "summary": "General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15849.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "672d96d0741fa214788392ef",
      "avatarUrl": "/avatars/651faa0f4a3ad0b67b584fd688063347.svg",
      "fullname": "Chou, TSUNG-HSIANG",
      "name": "yumeow122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63468c63b21bd5e350c1d177",
      "name": "NchuNLPLab",
      "fullname": "NCHU NLP Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1665567811412-6346516ffe134dfd7a120e00.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17323",
      "authors": [
        {
          "_id": "69783b4b026bdf0473116f0c",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:35:19.214Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f0d",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:35:27.008Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f0e",
          "user": {
            "_id": "643bedaf9f5d314db2da178d",
            "avatarUrl": "/avatars/a166dd66586f3600d95b725de6350bc1.svg",
            "isPro": false,
            "fullname": "tuanhuili",
            "user": "tuanhui",
            "type": "user"
          },
          "name": "Tuanhui Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:35:34.618Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f0f",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f10",
          "name": "Zheng Chen",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f11",
          "user": {
            "_id": "665402169ccb17d9678be0d2",
            "avatarUrl": "/avatars/9142d09c8d0e866eb688002d50fd3cbe.svg",
            "isPro": false,
            "fullname": "Jiangping Yang",
            "user": "JumpingXL",
            "type": "user"
          },
          "name": "Jiangping Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:35:53.130Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f12",
          "name": "Mingyuan Fan",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f13",
          "name": "Jingtao Xu",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f14",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f15",
          "name": "Baoxuan Gu",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f16",
          "user": {
            "_id": "65ab6d073e7c8dcbbeeeede8",
            "avatarUrl": "/avatars/fab2af24c653019665f1f8120fc3273b.svg",
            "isPro": false,
            "fullname": "Mingshan Chang",
            "user": "shesshan",
            "type": "user"
          },
          "name": "Mingshan Chang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:36:18.951Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f17",
          "name": "Yuqiang Xie",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f18",
          "name": "Binjie Mao",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f19",
          "name": "Youqiang Zhang",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1a",
          "name": "Nuo Pang",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1b",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1c",
          "name": "Yuzhe Jin",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1d",
          "user": {
            "_id": "65a932d909480af928b47b4c",
            "avatarUrl": "/avatars/2050dd33da157c12adc8f598175f4cd8.svg",
            "isPro": false,
            "fullname": "Zhiheng Xu",
            "user": "Zonnes",
            "type": "user"
          },
          "name": "Zhiheng Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:36:53.072Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1e",
          "name": "Dixuan Lin",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f1f",
          "user": {
            "_id": "6909e8b1d8dc3e811139c1da",
            "avatarUrl": "/avatars/30b14c15331ed6613456c9b3b4110e1b.svg",
            "isPro": false,
            "fullname": "Guibin Chen",
            "user": "jlu-cgb",
            "type": "user"
          },
          "name": "Guibin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:37:01.680Z",
          "hidden": false
        },
        {
          "_id": "69783b4b026bdf0473116f20",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-24T06:08:12.000Z",
      "submittedOnDailyAt": "2026-01-27T01:43:04.419Z",
      "title": "SkyReels-V3 Technique Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.",
      "upvotes": 6,
      "discussionId": "69783b4c026bdf0473116f21",
      "ai_summary": "SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.",
      "ai_keywords": [
        "diffusion Transformers",
        "multimodal in-context learning framework",
        "reference images-to-video synthesis",
        "video-to-video extension",
        "audio-guided video generation",
        "spatio-temporal consistency modeling",
        "large-scale video understanding",
        "talking avatar model",
        "key-frame inference paradigms",
        "cross frame pairing",
        "semantic rewriting",
        "multi-resolution joint optimization",
        "image video hybrid strategy"
      ],
      "organization": {
        "_id": "6522615d9334173c627b0efa",
        "name": "Skywork",
        "fullname": "Skywork",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
      }
    },
    "publishedAt": "2026-01-24T01:08:12.000Z",
    "title": "SkyReels-V3 Technique Report",
    "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17323.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 218,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6522615d9334173c627b0efa",
      "name": "Skywork",
      "fullname": "Skywork",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18202",
      "authors": [
        {
          "_id": "69783a29026bdf0473116f00",
          "user": {
            "_id": "619d59d765db93fe9d894d97",
            "avatarUrl": "/avatars/d9ce5e3ab62acdb669d0d9aaf328588e.svg",
            "isPro": false,
            "fullname": "Fangyuan Xu",
            "user": "fangyuan",
            "type": "user"
          },
          "name": "Fangyuan Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:13.461Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f01",
          "user": {
            "_id": "64c04feeb746fe51543c1b7a",
            "avatarUrl": "/avatars/77b87fca59a64c2463d652d454d78b13.svg",
            "isPro": false,
            "fullname": "Han",
            "user": "Rujun",
            "type": "user"
          },
          "name": "Rujun Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:23.311Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f02",
          "user": {
            "_id": "64dbe2aa978d1f9fafe978e7",
            "avatarUrl": "/avatars/5e046834aa1f3ec88fa0fb3e5e3f0825.svg",
            "isPro": false,
            "fullname": "Yanfei Chen",
            "user": "anmourchen",
            "type": "user"
          },
          "name": "Yanfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:30.277Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f03",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f04",
          "user": {
            "_id": "64dc08f9e7bc8544f9b1ac32",
            "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
            "isPro": false,
            "fullname": "I-Hung Hsu",
            "user": "alexhsu",
            "type": "user"
          },
          "name": "I-Hung Hsu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:39.995Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f05",
          "name": "Jun Yan",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f06",
          "name": "Vishy Tirumalashetty",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f07",
          "user": {
            "_id": "5fca68d111d483aef7d3b26c",
            "avatarUrl": "/avatars/f563d906f9caf52838df7d7fb31f7c90.svg",
            "isPro": false,
            "fullname": "Eunsol Choi",
            "user": "eunsol",
            "type": "user"
          },
          "name": "Eunsol Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:32:51.393Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f08",
          "user": {
            "_id": "68e3f17160026cedfe38266b",
            "avatarUrl": "/avatars/ebecb599788f53d98076d4e37a060f19.svg",
            "isPro": false,
            "fullname": "Tomas Pfister",
            "user": "tpfister",
            "type": "user"
          },
          "name": "Tomas Pfister",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:33:01.573Z",
          "hidden": false
        },
        {
          "_id": "69783a29026bdf0473116f09",
          "name": "Chen-Yu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T06:37:56.000Z",
      "submittedOnDailyAt": "2026-01-27T01:38:19.495Z",
      "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.",
      "upvotes": 6,
      "discussionId": "69783a2a026bdf0473116f0a",
      "ai_summary": "Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.",
      "ai_keywords": [
        "deep search agents",
        "question-answer pairs",
        "reasoning strategies",
        "intrinsic evaluation",
        "extrinsic evaluation",
        "synthetic data",
        "agent-based pipeline",
        "iterative refinement"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-01-26T01:37:56.000Z",
    "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "summary": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18202.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 218,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18577",
      "authors": [
        {
          "_id": "697837d0026bdf0473116ed3",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "697837d0026bdf0473116ed4",
          "user": {
            "_id": "66b57c77778c98d29446c8ec",
            "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg",
            "isPro": false,
            "fullname": "Taekyung Ki",
            "user": "taekyungki",
            "type": "user"
          },
          "name": "Taekyung Ki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:00:25.179Z",
          "hidden": false
        },
        {
          "_id": "697837d0026bdf0473116ed5",
          "user": {
            "_id": "65eaf5f185b81b33fc8653ed",
            "avatarUrl": "/avatars/a223129cd0258cb0fdf3356eba178bae.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harry9704",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:29:04.004Z",
          "hidden": false
        },
        {
          "_id": "697837d0026bdf0473116ed6",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:29:18.724Z",
          "hidden": false
        },
        {
          "_id": "697837d0026bdf0473116ed7",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:00:27.411Z",
          "hidden": false
        },
        {
          "_id": "697837d0026bdf0473116ed8",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Ffm5MpnJjIL7Rznm-he0z.mp4"
      ],
      "publishedAt": "2026-01-26T15:22:27.000Z",
      "submittedOnDailyAt": "2026-01-27T01:32:21.601Z",
      "title": "Self-Refining Video Sampling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.",
      "upvotes": 20,
      "discussionId": "697837d0026bdf0473116ed9",
      "projectPage": "https://agwmon.github.io/self-refine-video/",
      "githubRepo": "https://github.com/agwmon/self-refine-video",
      "githubRepoAddedBy": "user",
      "ai_summary": "Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.",
      "ai_keywords": [
        "video generators",
        "denoising autoencoder",
        "iterative inner-loop refinement",
        "self-refining video sampling",
        "uncertainty-aware refinement",
        "motion coherence",
        "physics alignment"
      ],
      "githubStars": 52
    },
    "publishedAt": "2026-01-26T10:22:27.000Z",
    "title": "Self-Refining Video Sampling",
    "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Ffm5MpnJjIL7Rznm-he0z.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18577.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 218,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18184",
      "authors": [
        {
          "_id": "697832bd026bdf0473116e89",
          "user": {
            "_id": "68e3ba91fc9f7cbda9a2be02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5h166wHkdQSG7IusNV5wT.png",
            "isPro": false,
            "fullname": "pengzhiliang",
            "user": "zhiliang2",
            "type": "user"
          },
          "name": "Zhiliang Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:29:35.447Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8a",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8b",
          "user": {
            "_id": "65e552cb4dbf9514fb0c3110",
            "avatarUrl": "/avatars/655dd9a36e85c1290855fb2c296472f8.svg",
            "isPro": false,
            "fullname": "Yaoyao Chang",
            "user": "YaoyaoChang",
            "type": "user"
          },
          "name": "Yaoyao Chang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:30:15.096Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8c",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8d",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8e",
          "user": {
            "_id": "672b6da69380700b60c92367",
            "avatarUrl": "/avatars/651b8bd4d1d6bbd047c6f0d6010a0ea3.svg",
            "isPro": false,
            "fullname": "Yingbo Hao",
            "user": "YingboHao",
            "type": "user"
          },
          "name": "Yingbo Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:30:33.802Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e8f",
          "name": "Yujie Tu",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e90",
          "name": "Chenyu Yang",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e91",
          "user": {
            "_id": "661396d69ef83c1509d41c3f",
            "avatarUrl": "/avatars/9760ac72f2d44320e6033d30e2ce7bd5.svg",
            "isPro": false,
            "fullname": "Wenhui Wang",
            "user": "stonewh1",
            "type": "user"
          },
          "name": "Wenhui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:30:44.557Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e92",
          "name": "Songchen Xu",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e93",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e94",
          "name": "Hangbo Bao",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e95",
          "user": {
            "_id": "66f4cc8dcb62f781535a27ac",
            "avatarUrl": "/avatars/2b6cf6513a0c93f932115634092f7d30.svg",
            "isPro": false,
            "fullname": "Weijiang Xu",
            "user": "WeijiangXU",
            "type": "user"
          },
          "name": "Weijiang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:30:57.860Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e96",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e97",
          "user": {
            "_id": "67b358e504fec7fd742ae8c1",
            "avatarUrl": "/avatars/3f404b20df769bcf87e8d8d7d5fc9ed6.svg",
            "isPro": false,
            "fullname": "Zehua Wang",
            "user": "zehuawang",
            "type": "user"
          },
          "name": "Zehua Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:31:04.270Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e98",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e99",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9a",
          "user": {
            "_id": "60f6d61f89b21b8fd2d471c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f6d61f89b21b8fd2d471c6/RmLFf97vUoXMoCT3rWbhm.jpeg",
            "isPro": false,
            "fullname": "Zewen Chi",
            "user": "CZWin32768",
            "type": "user"
          },
          "name": "Zewen Chi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:31:10.007Z",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9b",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9c",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9d",
          "name": "Chuang Ding",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9e",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116e9f",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "697832bd026bdf0473116ea0",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:31:19.998Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T06:11:51.000Z",
      "submittedOnDailyAt": "2026-01-27T01:18:16.537Z",
      "title": "VIBEVOICE-ASR Technical Report",
      "submittedOnDailyBy": {
        "_id": "67ecd6178647cfa1775f75ed",
        "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
        "isPro": false,
        "fullname": "Furu Wei",
        "user": "frontierai",
        "type": "user"
      },
      "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.",
      "upvotes": 14,
      "discussionId": "697832bd026bdf0473116ea1",
      "ai_summary": "VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.",
      "ai_keywords": [
        "speech understanding framework",
        "VibeVoice",
        "Automatic Speech Recognition",
        "Speaker Diarization",
        "Timestamping",
        "end-to-end generation",
        "long-form audio",
        "single-pass processing",
        "multilingual support",
        "code-switching",
        "prompt-based context injection"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-01-26T01:11:51.000Z",
    "title": "VIBEVOICE-ASR Technical Report",
    "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18184.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "67ecd6178647cfa1775f75ed",
      "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
      "fullname": "Furu Wei",
      "name": "frontierai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.17737",
      "authors": [
        {
          "_id": "6978310b026bdf0473116e44",
          "user": {
            "_id": "64545c77a7ce0a8fde809912",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg",
            "isPro": false,
            "fullname": "ChenYuMu",
            "user": "ChenYuMu",
            "type": "user"
          },
          "name": "Chenyu Mu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:01:21.462Z",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e45",
          "user": {
            "_id": "6527a2df1eb78901534b0cc6",
            "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg",
            "isPro": false,
            "fullname": "Xin He",
            "user": "Kleinhe",
            "type": "user"
          },
          "name": "Xin He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:20.414Z",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e46",
          "user": {
            "_id": "64300415b009240418dac70c",
            "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg",
            "isPro": false,
            "fullname": "Qu Yang",
            "user": "quyang22",
            "type": "user"
          },
          "name": "Qu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:03:22.475Z",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e47",
          "name": "Wanshun Chen",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e48",
          "name": "Jiadi Yao",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e49",
          "name": "Huang Liu",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4a",
          "name": "Zihao Yi",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4b",
          "name": "Bo Zhao",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4c",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4d",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4e",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e4f",
          "name": "Erkun Yang",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e50",
          "name": "Cheng Deng",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e51",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e52",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "6978310b026bdf0473116e53",
          "name": "Linus",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-25T08:10:28.000Z",
      "submittedOnDailyAt": "2026-01-27T01:05:46.612Z",
      "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
      "submittedOnDailyBy": {
        "_id": "67485743561b1e6f9579389f",
        "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
        "isPro": false,
        "fullname": "Zhaopeng Tu",
        "user": "zptu",
        "type": "user"
      },
      "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
      "upvotes": 48,
      "discussionId": "6978310b026bdf0473116e54",
      "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/",
      "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.",
      "ai_keywords": [
        "video generation",
        "dialogue-to-cinematic-video",
        "ScripterAgent",
        "DirectorAgent",
        "cross-scene continuous generation",
        "ScriptBench",
        "Visual-Script Alignment",
        "CriticAgent"
      ],
      "githubStars": 240,
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2026-01-25T03:10:28.000Z",
    "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "67485743561b1e6f9579389f",
      "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
      "fullname": "Zhaopeng Tu",
      "name": "zptu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.18081",
      "authors": [
        {
          "_id": "6978308a026bdf0473116e36",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638d601b5e14c2f38678fb3a/Elu-TTd97lGy7YL7eKRuZ.jpeg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:42:34.672Z",
          "hidden": false
        },
        {
          "_id": "6978308a026bdf0473116e37",
          "name": "Yingjie Yu",
          "hidden": false
        },
        {
          "_id": "6978308a026bdf0473116e38",
          "name": "Jingjun Xu",
          "hidden": false
        },
        {
          "_id": "6978308a026bdf0473116e39",
          "user": {
            "_id": "68acb86d5c1a878ddb38210f",
            "avatarUrl": "/avatars/7553021c5e1f7494752e06b0c8d3d7f5.svg",
            "isPro": false,
            "fullname": "jiaxuanYou",
            "user": "jiaxuanYou",
            "type": "user"
          },
          "name": "Jiaxuan You",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:34:15.055Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638d601b5e14c2f38678fb3a/3lq04lzJhs3yMr8OYwHM_.png"
      ],
      "publishedAt": "2026-01-26T02:30:01.000Z",
      "submittedOnDailyAt": "2026-01-27T00:58:02.568Z",
      "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
      "submittedOnDailyBy": {
        "_id": "638d601b5e14c2f38678fb3a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638d601b5e14c2f38678fb3a/Elu-TTd97lGy7YL7eKRuZ.jpeg",
        "isPro": false,
        "fullname": "韩沛煊",
        "user": "HakHan",
        "type": "user"
      },
      "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.",
      "upvotes": 7,
      "discussionId": "6978308a026bdf0473116e3a",
      "githubRepo": "https://github.com/ulab-uiuc/DRPG-RebuttalAgent/tree/master",
      "githubRepoAddedBy": "user",
      "ai_summary": "An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.",
      "ai_keywords": [
        "large language models",
        "academic rebuttal",
        "agentic framework",
        "long-context understanding",
        "automated reasoning",
        "multi-perspective analysis",
        "explainable AI"
      ],
      "githubStars": 4
    },
    "publishedAt": "2026-01-25T21:30:01.000Z",
    "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
    "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638d601b5e14c2f38678fb3a/3lq04lzJhs3yMr8OYwHM_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18081.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "638d601b5e14c2f38678fb3a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638d601b5e14c2f38678fb3a/Elu-TTd97lGy7YL7eKRuZ.jpeg",
      "fullname": "韩沛煊",
      "name": "HakHan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.17367",
      "authors": [
        {
          "_id": "6978295b026bdf0473116db5",
          "user": {
            "_id": "64096ef79e9f790c905b846d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
            "isPro": false,
            "fullname": "Zecheng Tang",
            "user": "ZetangForward",
            "type": "user"
          },
          "name": "Zecheng Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-27T09:42:48.253Z",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116db6",
          "user": {
            "_id": "6732fb1d24b316be87acaafe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg",
            "isPro": false,
            "fullname": "Quantong Qiu",
            "user": "QQTang1223",
            "type": "user"
          },
          "name": "Quantong Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:27:08.590Z",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116db7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116db8",
          "name": "Zhiyi Hong",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116db9",
          "name": "Haiya Xiang",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116dba",
          "user": {
            "_id": "67e7e66d0d9434ad14575739",
            "avatarUrl": "/avatars/5cf522a690519c22b65b959c9e0ca8d2.svg",
            "isPro": false,
            "fullname": "Kebin Liu",
            "user": "wanderhzz",
            "type": "user"
          },
          "name": "Kebin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-28T11:32:06.877Z",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116dbb",
          "user": {
            "_id": "68cce9276e4618473d590342",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UqBWbWZ58EPwJ5iRUsJsB.png",
            "isPro": false,
            "fullname": "Qingqing Dang",
            "user": "DaisyGrace",
            "type": "user"
          },
          "name": "Qingqing Dang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:27:50.734Z",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116dbc",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6670e285b0c03c4e9d6e0985/j9Zr-lOtrRmpFz5f4x420.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-01-27T14:27:56.908Z",
          "hidden": false
        },
        {
          "_id": "6978295b026bdf0473116dbd",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-24T08:22:07.000Z",
      "submittedOnDailyAt": "2026-01-27T00:51:37.565Z",
      "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
      "submittedOnDailyBy": {
        "_id": "64096ef79e9f790c905b846d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
        "isPro": false,
        "fullname": "Zecheng Tang",
        "user": "ZetangForward",
        "type": "user"
      },
      "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.",
      "upvotes": 29,
      "discussionId": "6978295b026bdf0473116dbe",
      "projectPage": "https://github.com/LCM-Lab/Elastic-Attention",
      "githubRepo": "https://github.com/LCM-Lab/Elastic-Attention",
      "githubRepoAddedBy": "user",
      "ai_summary": "Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.",
      "ai_keywords": [
        "standard attention mechanisms",
        "sparse attention",
        "full attention",
        "hybrid attention strategies",
        "attention router",
        "attention heads",
        "long-context scenarios",
        "pretrained models",
        "computational efficiency"
      ],
      "githubStars": 13,
      "organization": {
        "_id": "61f8e653129c9ff1b911293d",
        "name": "SUDA",
        "fullname": "Soochow University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
      }
    },
    "publishedAt": "2026-01-24T03:22:07.000Z",
    "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
    "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17367.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64096ef79e9f790c905b846d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
      "fullname": "Zecheng Tang",
      "name": "ZetangForward",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61f8e653129c9ff1b911293d",
      "name": "SUDA",
      "fullname": "Soochow University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
    },
    "isAuthorParticipating": true
  }
]
